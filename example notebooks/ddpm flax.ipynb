{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from typing import Dict, Callable, Sequence, Any, Union\n",
    "from dataclasses import field\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "import optax\n",
    "from flax import struct                # Flax dataclasses\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from flax.training import orbax_utils\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Important Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeImage = lambda x: jax.nn.standardize(x, mean=[127.5], std=[127.5])\n",
    "denormalizeImage = lambda x: (x + 1.0) * 127.5\n",
    "\n",
    "\n",
    "def plotImages(imgs, fig_size=(8, 8), dpi=100):\n",
    "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
    "    imglen = imgs.shape[0]\n",
    "    for i in range(imglen):\n",
    "        plt.subplot(fig_size[0], fig_size[1], i + 1)\n",
    "        plt.imshow(tf.cast(denormalizeImage(imgs[i, :, :, :]), tf.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "class RandomClass():\n",
    "    def __init__(self, rng: jax.random.PRNGKey):\n",
    "        self.rng = rng\n",
    "\n",
    "    def get_random_key(self):\n",
    "        self.rng, subkey = jax.random.split(self.rng)\n",
    "        return subkey\n",
    "    \n",
    "    def get_sigmas(self, steps):\n",
    "        return jnp.tan(self.theta_min + steps * (self.theta_max - self.theta_min)) / self.kappa\n",
    "\n",
    "    def reset_random_key(self):\n",
    "        self.rng = jax.random.PRNGKey(42)\n",
    "\n",
    "class MarkovState(struct.PyTreeNode):\n",
    "    pass\n",
    "\n",
    "class RandomMarkovState(MarkovState):\n",
    "    rng: jax.random.PRNGKey\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rng, subkey = jax.random.split(self.rng)\n",
    "        return RandomMarkovState(rng), subkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_name=\"celeb_a\", batch_size=64, image_scale=256):\n",
    "    def augmenter(image_scale=256, method=\"area\"):\n",
    "        @tf.function()\n",
    "        def augment(sample):\n",
    "            image = (\n",
    "                tf.cast(sample[\"image\"], tf.float32) - 127.5\n",
    "            ) / 127.5\n",
    "            image = tf.image.resize(\n",
    "                image, [image_scale, image_scale], method=method, antialias=True\n",
    "            )\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_contrast(image, 0.999, 1.05)\n",
    "            image = tf.image.random_brightness(image, 0.2)\n",
    "\n",
    "            image = tf.clip_by_value(image, -1.0, 1.0)\n",
    "            return image\n",
    "        return augment\n",
    "\n",
    "    # Load CelebA Dataset\n",
    "    data: tf.data.Dataset = tfds.load(data_name, split=\"all\", shuffle_files=True)\n",
    "    final_data = (\n",
    "        data\n",
    "        .cache()  # Cache after augmenting to avoid recomputation\n",
    "        .map(\n",
    "            augmenter(image_scale, method=\"area\"),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .repeat()  # Repeats the dataset indefinitely\n",
    "        .shuffle(4096)  # Ensure this is adequate for your dataset size\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    ).as_numpy_iterator()\n",
    "    return final_data, len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Schedulers\n",
    "\n",
    "A Noise schedule governs how noise is added in the forward diffusion steps. Basically, given a time step $t$, it \n",
    "returns the signal rate $\\alpha$ and noise rate $\\sigma_t$ with which to scale the initial data sample $x_0$ and Gaussian noise $\\epsilon$ as given by the equation:\n",
    "\n",
    "$x_t = \\alpha_t * x_0 + \\sigma_t * \\epsilon_0$\n",
    "\n",
    "where $x_t$ is the data sample at time $t$, $x_0$ is the initial data sample, and $\\epsilon$ is the Gaussian noise, and $\\alpha$ and $\\sigma_t$ are the signal and noise rates at time $t$ respectively.\n",
    "\n",
    "In variance preserving diffusion, the noise schedule is such that the variance of the data sample remains constant across time steps. This basically means the following:\n",
    "\n",
    "$\\alpha_t^2 + \\sigma_t^2 = 1$\n",
    "\n",
    "The idea is that with increasing time step $t$, the signal rate $\\alpha_t$ decreases and the noise rate $\\sigma_t$ increases, decreasing the $%$ of the initial data sample and increasing the amount of noise, slowly diffusing the data sample smoothly to the target normal distribution.\n",
    "\n",
    "Ofcourse there are many ways to schedule the noise, and the constraint of variance preserving isn't the only way to go about it either. There are Variance exploding and variance preserving schedules as well. In this notebook, we will be looking at the variance preserving noise schedules only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler():\n",
    "    def __init__(self, timesteps,\n",
    "                    dtype=jnp.float32,\n",
    "                    clip_min=-1.0,\n",
    "                    clip_max=1.0,\n",
    "                    *args, **kwargs):\n",
    "        self.max_timesteps = timesteps\n",
    "        self.dtype = dtype\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "\n",
    "    def generate_timesteps(self, batch_size, state:RandomMarkovState) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_weights(self, steps):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reshape_rates(self, rates:tuple[jnp.ndarray, jnp.ndarray], shape=(-1, 1, 1, 1)) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        alpha, sigma = rates\n",
    "        alpha = jnp.reshape(alpha, shape)\n",
    "        sigma = jnp.reshape(sigma, shape)\n",
    "        return alpha, sigma\n",
    "    \n",
    "    def get_rates(self, steps, shape=(-1, 1, 1, 1)) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def add_noise(self, images, noise, steps) -> jnp.ndarray:\n",
    "        alpha, sigma = self.get_rates(steps)\n",
    "        return alpha * images + sigma * noise\n",
    "    \n",
    "    def remove_all_noise(self, noisy_images, noise, steps, clip_denoised=True, rates=None) -> jnp.ndarray:\n",
    "        alpha, sigma = self.get_rates(steps)\n",
    "        x_0 = (noisy_images - noise * sigma) / alpha\n",
    "        return x_0\n",
    "    \n",
    "    def transform_inputs(self, x, steps) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        return x, steps\n",
    "    \n",
    "    def get_posterior_mean(self, x_0, x_t, steps):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_posterior_variance(self, steps, shape=(-1, 1, 1, 1)):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_max_variance(self):\n",
    "        alpha_n, sigma_n = self.get_rates(self.max_timesteps)\n",
    "        variance = jnp.sqrt(alpha_n ** 2 + sigma_n ** 2) \n",
    "        return variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Noise Scheduler\n",
    "\n",
    "Cosine Schedule is one of the most widely used noise schedules. Its a Variance Preserving noise schedule and can either be parameterized directly as the functions $\\alpha(t)$ and $\\sigma(t)$ defined directly as \n",
    "\n",
    "$cos(\\dfrac{\\pi t}{2 T})$ and $sin(\\dfrac{\\pi t}{2 T})$, \n",
    "\n",
    "which is simple to understand as $sin(x)^2 + cos(x)^2 = 1$, which satisfies the variance preserving constraint, \n",
    "or in terms of functions depending on a $\\beta(t)$ parameter as defined in the original DDPM paper and many more places. \n",
    "\n",
    "You see, In the original DDPM Paper, the forward diffusion step is defined as \n",
    "\n",
    "$q(x_t|x_{t−1}) := \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t−1}, \\beta_t I)$\n",
    "\n",
    "where $q(x_t|x_{t−1})$ is the forward diffusion step. It's the conditional distribution of the data sample $x_t$ given the previous data sample $x_{t-1}$, and the equation states that $x_t$ is normally distributed with mean $\\sqrt{1-\\beta_t}x_{t−1}$ and variance $\\beta_t$.\n",
    "\n",
    "In human speak, the forward diffusion is phrased as:\n",
    "\n",
    "$x_t = \\sqrt{1-\\beta_t}x_{t−1} + \\sqrt{\\beta_t} \\epsilon_t$\n",
    "\n",
    "where $x_t$ is the data sample at time $t$, $x_{t-1}$ is the previous data sample, and $\\epsilon_t$ is the Gaussian noise at time $t$. Notice that $x_t$ is phrased in terms of $x_{t-1}$ instead of the initial data sample $x_0$ as we did in the 'Noise Schedulers' section. \n",
    "\n",
    "To convert this formulation to the type \n",
    "$x_t = \\alpha_t  x_0 + \\sigma_t  \\epsilon_0$,\n",
    "\n",
    "we can find that our signal rate $\\alpha_t$ and noise rate $\\sigma_t$ are given by:\n",
    "\n",
    "$\\alpha_t = \\prod_t \\sqrt{1-\\beta_t}$\n",
    "\n",
    "$\\sigma_t = \\sqrt{1-\\alpha_t^2}$ \n",
    "\n",
    "The thing is, the maths in DDPM paper use the symbol $\\alpha$ for a different thing, as an intermediate to denote the value $\\alpha_t = 1-\\beta_t$, so just be careful with the notation. We use $\\alpha$ to denote the signal rate in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContinuousNoiseScheduler(NoiseScheduler):\n",
    "    \"\"\"\n",
    "    General Continuous Noise Scheduler\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(timesteps=1, *args, **kwargs)\n",
    "\n",
    "class CosineContinuousNoiseScheduler(ContinuousNoiseScheduler):\n",
    "    def get_rates(self, steps, shape=(-1, 1, 1, 1)) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        signal_rates = jnp.cos((jnp.pi * steps) / (2 * self.max_timesteps))\n",
    "        noise_rates = jnp.sin((jnp.pi * steps) / (2 * self.max_timesteps))\n",
    "        return self.reshape_rates((signal_rates, noise_rates), shape=shape)\n",
    "    \n",
    "    def get_weights(self, steps):\n",
    "        alpha, sigma = self.get_rates(steps, shape=())\n",
    "        return 1 / (1 + (alpha ** 2 / sigma ** 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
