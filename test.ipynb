{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/polynomial/__init__.py:116\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mA sub-package for efficiently dealing with polynomials.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolynomial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Polynomial\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchebyshev\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chebyshev\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegendre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Legendre\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/polynomial/polynomial.py:87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_axis_index\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polyutils \u001b[38;5;28;01mas\u001b[39;00m pu\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_polybase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABCPolyBase\n\u001b[1;32m     89\u001b[0m polytrim \u001b[38;5;241m=\u001b[39m pu\u001b[38;5;241m.\u001b[39mtrimcoef\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# These are constant arrays are of integer type so as to be compatible\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# with the widest range of other types, such as Decimal.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Polynomial default domain.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1073\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "_multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: _multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linen \u001b[38;5;28;01mas\u001b[39;00m nn\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m config: configurations\u001b[38;5;241m.\u001b[39mConfig \u001b[38;5;241m=\u001b[39m configurations\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m configurations\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_utils\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linen\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/core/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2024 The Flax Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxes_scan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m broadcast \u001b[38;5;28;01mas\u001b[39;00m broadcast\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrozen_dict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     FrozenDict \u001b[38;5;28;01mas\u001b[39;00m FrozenDict,\n\u001b[1;32m     18\u001b[0m     copy \u001b[38;5;28;01mas\u001b[39;00m copy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     unfreeze \u001b[38;5;28;01mas\u001b[39;00m unfreeze,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlift\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     custom_vjp \u001b[38;5;28;01mas\u001b[39;00m custom_vjp,\n\u001b[1;32m     26\u001b[0m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     while_loop \u001b[38;5;28;01mas\u001b[39;00m while_loop,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/core/axes_scan.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/__init__.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version_info__ \u001b[38;5;28;01mas\u001b[39;00m __version_info__\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Set Cloud TPU env vars if necessary before transitively loading C++ backend\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud_tpu_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cloud_tpu_init \u001b[38;5;28;01mas\u001b[39;00m _cloud_tpu_init\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m   _cloud_tpu_init()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/cloud_tpu_init.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hardware_utils\n\u001b[1;32m     20\u001b[0m running_in_cloud_tpu_vm: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/config.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Generic, NamedTuple, NoReturn, Protocol, TypeVar, cast\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_jit\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lib/__init__.py:87\u001b[0m\n\u001b[1;32m     84\u001b[0m cpu_feature_guard\u001b[38;5;241m.\u001b[39mcheck_cpu_features()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxla_client\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxla_client\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlapack\u001b[39;00m\n\u001b[1;32m     90\u001b[0m xla_extension \u001b[38;5;241m=\u001b[39m xla_client\u001b[38;5;241m.\u001b[39m_xla\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jaxlib/xla_client.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Protocol, Union\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xla_extension \u001b[38;5;28;01mas\u001b[39;00m _xla\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ml_dtypes/__init__.py:32\u001b[0m\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ml_dtypes/_finfo.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Overload of numpy.finfo to handle dtypes defined in ml_dtypes.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3b11fnuz\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3fn\n",
      "\u001b[0;31mImportError\u001b[0m: numpy._core.umath failed to import"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import flax\n",
    "import tqdm\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from typing import Dict, Callable, Sequence, Any, Union\n",
    "from dataclasses import field\n",
    "import jax.numpy as jnp\n",
    "import grain.python as pygrain\n",
    "import numpy as np\n",
    "import augmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "import optax\n",
    "from flax import struct                # Flax dataclasses\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from flax.training import orbax_utils\n",
    "import functools\n",
    "\n",
    "import json\n",
    "# For CLIP\n",
    "from transformers import AutoTokenizer, FlaxCLIPTextModel, CLIPTextModel\n",
    "import wandb\n",
    "\n",
    "import argparse\n",
    "\n",
    "# %% [markdown]\n",
    "# # Initialization\n",
    "#####################################################################################################################\n",
    "################################################# Initialization ####################################################\n",
    "#####################################################################################################################\n",
    "\n",
    "# %%\n",
    "jax.distributed.initialize() \n",
    "\n",
    "# %%\n",
    "print(f\"Number of devices: {jax.device_count()}\")\n",
    "print(f\"Local devices: {jax.local_devices()}\")\n",
    "\n",
    "# %%\n",
    "normalizeImage = lambda x: jax.nn.standardize(x, mean=[127.5], std=[127.5])\n",
    "denormalizeImage = lambda x: (x + 1.0) * 127.5\n",
    "\n",
    "\n",
    "def plotImages(imgs, fig_size=(8, 8), dpi=100):\n",
    "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
    "    imglen = imgs.shape[0]\n",
    "    for i in range(imglen):\n",
    "        plt.subplot(fig_size[0], fig_size[1], i + 1)\n",
    "        plt.imshow(jnp.astype(denormalizeImage(imgs[i, :, :, :]), jnp.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "class RandomClass():\n",
    "    def __init__(self, rng: jax.random.PRNGKey):\n",
    "        self.rng = rng\n",
    "\n",
    "    def get_random_key(self):\n",
    "        self.rng, subkey = jax.random.split(self.rng)\n",
    "        return subkey\n",
    "    \n",
    "    def get_sigmas(self, steps):\n",
    "        return jnp.tan(self.theta_min + steps * (self.theta_max - self.theta_min)) / self.kappa\n",
    "\n",
    "    def reset_random_key(self):\n",
    "        self.rng = jax.random.PRNGKey(42)\n",
    "\n",
    "class MarkovState(struct.PyTreeNode):\n",
    "    pass\n",
    "\n",
    "class RandomMarkovState(MarkovState):\n",
    "    rng: jax.random.PRNGKey\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rng, subkey = jax.random.split(self.rng)\n",
    "        return RandomMarkovState(rng), subkey\n",
    "\n",
    "# %% [markdown]\n",
    "# # Data Pipeline\n",
    "\n",
    "# %%\n",
    "def defaultTextEncodeModel(backend=\"jax\"):\n",
    "    modelname = \"openai/clip-vit-large-patch14\"\n",
    "    if backend == \"jax\":\n",
    "        model = FlaxCLIPTextModel.from_pretrained(modelname, dtype=jnp.bfloat16)\n",
    "    else:\n",
    "        model = CLIPTextModel.from_pretrained(modelname)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname, dtype=jnp.float16)\n",
    "    return model, tokenizer\n",
    "    \n",
    "def encodePrompts(prompts, model, tokenizer=None):\n",
    "    if model == None:\n",
    "        model, tokenizer = defaultTextEncodeModel()\n",
    "    if tokenizer == None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"np\")\n",
    "    inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"np\")\n",
    "    outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "    # outputs = infer(inputs['input_ids'], inputs['attention_mask'])\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooler_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "    embed_pooled = pooler_output#.astype(jnp.float16)\n",
    "    embed_labels_full = last_hidden_state#.astype(jnp.float16)\n",
    "    \n",
    "    return embed_pooled, embed_labels_full\n",
    "\n",
    "class CaptionProcessor:\n",
    "    def __init__(self, tensor_type=\"pt\", modelname=\"openai/clip-vit-large-patch14\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "        self.tensor_type = tensor_type\n",
    "        \n",
    "    def __call__(self, caption):\n",
    "        # print(caption)\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=self.tensor_type)\n",
    "        # print(tokens.keys())\n",
    "        return {\n",
    "            \"input_ids\": tokens[\"input_ids\"],\n",
    "            \"attention_mask\": tokens[\"attention_mask\"],\n",
    "            \"caption\": caption,\n",
    "        }\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "# %%\n",
    "def data_source_tfds(name):\n",
    "    import tensorflow_datasets as tfds\n",
    "    def data_source():\n",
    "        return tfds.load(name, split=\"all\", shuffle_files=True)\n",
    "    return data_source\n",
    "\n",
    "def data_source_cc12m(source=\"/home/mrwhite0racle/research/FlaxDiff/datasets/gcs_mount/arrayrecord/cc12m/\"):\n",
    "    def data_source():\n",
    "        cc12m_records_path = source\n",
    "        cc12m_records = [os.path.join(cc12m_records_path, i) for i in os.listdir(cc12m_records_path) if 'array_record' in i]\n",
    "        ds = pygrain.ArrayRecordDataSource(cc12m_records[:-1])\n",
    "        return ds\n",
    "    return data_source\n",
    "\n",
    "def labelizer_oxford_flowers102(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        textlabels = [i.strip() for i in f.readlines()]\n",
    "    textlabels = tf.convert_to_tensor(textlabels)\n",
    "    def load_labels(sample):\n",
    "        return textlabels[sample['label']]\n",
    "    return load_labels\n",
    "\n",
    "def labelizer_cc12m(sample):\n",
    "    return sample['txt']\n",
    "\n",
    "# Configure the following for your datasets\n",
    "datasetMap = {\n",
    "    \"oxford_flowers102\": {\n",
    "        \"source\":data_source_tfds(\"oxford_flowers102\"),\n",
    "        \"labelizer\":lambda : labelizer_oxford_flowers102(\"/home/mrwhite0racle/tensorflow_datasets/oxford_flowers102/2.1.1/label.labels.txt\"),\n",
    "    },\n",
    "    \"cc12m\": {\n",
    "        \"source\":data_source_cc12m(),\n",
    "        \"labelizer\":lambda : labelizer_cc12m,\n",
    "    }\n",
    "}\n",
    "\n",
    "# %%\n",
    "import struct as st\n",
    "\n",
    "def unpack_dict_of_byte_arrays(packed_data):\n",
    "    unpacked_dict = {}\n",
    "    offset = 0\n",
    "    while offset < len(packed_data):\n",
    "        # Unpack the key length\n",
    "        key_length = st.unpack_from('I', packed_data, offset)[0]\n",
    "        offset += st.calcsize('I')\n",
    "        # Unpack the key bytes and convert to string\n",
    "        key = packed_data[offset:offset+key_length].decode('utf-8')\n",
    "        offset += key_length\n",
    "        # Unpack the byte array length\n",
    "        byte_array_length = st.unpack_from('I', packed_data, offset)[0]\n",
    "        offset += st.calcsize('I')\n",
    "        # Unpack the byte array\n",
    "        byte_array = packed_data[offset:offset+byte_array_length]\n",
    "        offset += byte_array_length\n",
    "        unpacked_dict[key] = byte_array\n",
    "    return unpacked_dict\n",
    "\n",
    "def get_dataset_grain(data_name=\"oxford_flowers102\", \n",
    "                      batch_size=64, image_scale=256, \n",
    "                      count=None, num_epochs=None,\n",
    "                      text_encoders=defaultTextEncodeModel(), \n",
    "                      method=jax.image.ResizeMethod.LANCZOS3,\n",
    "                      grain_worker_count=32, grain_read_thread_count=64, \n",
    "                      grain_read_buffer_size=50, grain_worker_buffer_size=20):\n",
    "    dataset = datasetMap[data_name]\n",
    "    data_source = dataset[\"source\"]()\n",
    "    labelizer = dataset[\"labelizer\"]()\n",
    "    \n",
    "    import cv2\n",
    "    \n",
    "    model, tokenizer = text_encoders\n",
    "\n",
    "    null_labels, null_labels_full = encodePrompts([\"\"], model, tokenizer)\n",
    "    null_labels = np.array(null_labels[0], dtype=np.float16)\n",
    "    null_labels_full  = np.array(null_labels_full[0], dtype=np.float16)\n",
    "\n",
    "    class augmenter(pygrain.MapTransform):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.caption_processor = CaptionProcessor(tensor_type=\"np\")\n",
    "            \n",
    "        def map(self, element) ->  Dict[str, jnp.array]:\n",
    "            element = unpack_dict_of_byte_arrays(element)\n",
    "            image = np.asarray(bytearray(element['jpg']), dtype=\"uint8\")\n",
    "            image = cv2.imdecode(image, cv2.IMREAD_UNCHANGED)\n",
    "            image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (image_scale, image_scale), interpolation=cv2.INTER_AREA)\n",
    "            # image = (image - 127.5) / 127.5\n",
    "            caption = labelizer(element).decode('utf-8')\n",
    "            results = self.caption_processor(caption)\n",
    "            return {\n",
    "                    \"image\": image,\n",
    "                    \"input_ids\": results['input_ids'][0],\n",
    "                    \"attention_mask\": results['attention_mask'][0],\n",
    "                } \n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        num_records=len(data_source) if count is None else count,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        num_epochs=num_epochs,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "    )\n",
    "\n",
    "    transformations = [augmenter(), pygrain.Batch(batch_size, drop_remainder=True)]\n",
    "\n",
    "    loader = pygrain.DataLoader(\n",
    "        data_source=data_source,\n",
    "        sampler=sampler,\n",
    "        operations=transformations,\n",
    "        worker_count=grain_worker_count,\n",
    "        read_options=pygrain.ReadOptions(grain_read_thread_count, grain_read_buffer_size),\n",
    "        worker_buffer_size=grain_worker_buffer_size\n",
    "        )\n",
    "    \n",
    "    def get_trainset():\n",
    "        return loader\n",
    "    \n",
    "    return {\n",
    "        \"train\": get_trainset,\n",
    "        \"train_len\": len(data_source),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"null_labels\": null_labels,\n",
    "        \"null_labels_full\": null_labels_full,\n",
    "        \"model\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "    }\n",
    "\n",
    "# %%\n",
    "from flaxdiff.schedulers import CosineNoiseSchedule, NoiseScheduler, GeneralizedNoiseScheduler, KarrasVENoiseScheduler, EDMNoiseScheduler\n",
    "from flaxdiff.predictors import VPredictionTransform, EpsilonPredictionTransform, DiffusionPredictionTransform, DirectPredictionTransform, KarrasPredictionTransform\n",
    "\n",
    "# %%\n",
    "import jax.experimental.pallas.ops.tpu.flash_attention\n",
    "from flaxdiff.models.simple_unet import ConvLayer, TimeProjection, Upsample, Downsample, ResidualBlock\n",
    "from flaxdiff.models.simple_unet import FourierEmbedding\n",
    "\n",
    "from flaxdiff.models.attention import kernel_init, TransformerBlock\n",
    "\n",
    "# Kernel initializer to use\n",
    "def kernel_init(scale, dtype=jnp.float32):\n",
    "    scale = max(scale, 1e-10)\n",
    "    return nn.initializers.variance_scaling(scale=scale, mode=\"fan_avg\", distribution=\"truncated_normal\", dtype=dtype)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    emb_features:int=64*4,\n",
    "    feature_depths:list=[64, 128, 256, 512],\n",
    "    attention_configs:list=[{\"heads\":8}, {\"heads\":8}, {\"heads\":8}, {\"heads\":8}],\n",
    "    num_res_blocks:int=2,\n",
    "    num_middle_res_blocks:int=1,\n",
    "    activation:Callable = jax.nn.swish\n",
    "    norm_groups:int=32\n",
    "    dtype: Any = jnp.bfloat16\n",
    "    precision: Any = jax.lax.Precision.HIGH\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, temb, textcontext=None):\n",
    "        # print(\"embedding features\", self.emb_features)\n",
    "        temb = FourierEmbedding(features=self.emb_features)(temb)\n",
    "        temb = TimeProjection(features=self.emb_features)(temb)\n",
    "        \n",
    "        _, TS, TC = textcontext.shape\n",
    "        \n",
    "        # print(\"time embedding\", temb.shape)\n",
    "        feature_depths = self.feature_depths\n",
    "        attention_configs = self.attention_configs\n",
    "\n",
    "        conv_type = up_conv_type = down_conv_type = middle_conv_type = \"conv\"\n",
    "        # middle_conv_type = \"separable\"\n",
    "\n",
    "        x = ConvLayer(\n",
    "            conv_type,\n",
    "            features=self.feature_depths[0],\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            kernel_init=kernel_init(1.0),\n",
    "            dtype=self.dtype,\n",
    "            precision=self.precision\n",
    "        )(x)\n",
    "        downs = [x]\n",
    "\n",
    "        # Downscaling blocks\n",
    "        for i, (dim_out, attention_config) in enumerate(zip(feature_depths, attention_configs)):\n",
    "            dim_in = x.shape[-1]\n",
    "            # dim_in = dim_out\n",
    "            for j in range(self.num_res_blocks):\n",
    "                x = ResidualBlock(\n",
    "                    down_conv_type,\n",
    "                    name=f\"down_{i}_residual_{j}\",\n",
    "                    features=dim_in,\n",
    "                    kernel_init=kernel_init(1.0),\n",
    "                    kernel_size=(3, 3),\n",
    "                    strides=(1, 1),\n",
    "                    activation=self.activation,\n",
    "                    norm_groups=self.norm_groups,\n",
    "                    dtype=self.dtype,\n",
    "                    precision=self.precision\n",
    "                )(x, temb)\n",
    "                if attention_config is not None and j == self.num_res_blocks - 1:   # Apply attention only on the last block\n",
    "                    B, H, W, _ = x.shape\n",
    "                    if H > TS:\n",
    "                        padded_context = jnp.pad(textcontext, ((0, 0), (0, H - TS), (0, 0)), mode='constant', constant_values=0).reshape((B, 1, H, TC))\n",
    "                    else:\n",
    "                        padded_context = None\n",
    "                    x = TransformerBlock(heads=attention_config['heads'], dtype=attention_config.get('dtype', jnp.float32),\n",
    "                                       dim_head=dim_in // attention_config['heads'],\n",
    "                                       use_flash_attention=attention_config.get(\"flash_attention\", True),\n",
    "                                       use_projection=attention_config.get(\"use_projection\", False),\n",
    "                                       use_self_and_cross=attention_config.get(\"use_self_and_cross\", True),\n",
    "                                       precision=attention_config.get(\"precision\", self.precision),\n",
    "                                       name=f\"down_{i}_attention_{j}\")(x, padded_context)\n",
    "                # print(\"down residual for feature level\", i, \"is of shape\", x.shape, \"features\", dim_in)\n",
    "                downs.append(x)\n",
    "            if i != len(feature_depths) - 1:\n",
    "                # print(\"Downsample\", i, x.shape)\n",
    "                x = Downsample(\n",
    "                    features=dim_out,\n",
    "                    scale=2,\n",
    "                    activation=self.activation,\n",
    "                    name=f\"down_{i}_downsample\",\n",
    "                    dtype=self.dtype,\n",
    "                    precision=self.precision\n",
    "                )(x)\n",
    "\n",
    "        # Middle Blocks\n",
    "        middle_dim_out = self.feature_depths[-1]\n",
    "        middle_attention = self.attention_configs[-1]\n",
    "        for j in range(self.num_middle_res_blocks):\n",
    "            x = ResidualBlock(\n",
    "                middle_conv_type,\n",
    "                name=f\"middle_res1_{j}\",\n",
    "                features=middle_dim_out,\n",
    "                kernel_init=kernel_init(1.0),\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                activation=self.activation,\n",
    "                norm_groups=self.norm_groups,\n",
    "                dtype=self.dtype,\n",
    "                precision=self.precision\n",
    "            )(x, temb)\n",
    "            if middle_attention is not None and j == self.num_middle_res_blocks - 1:   # Apply attention only on the last block\n",
    "                x = TransformerBlock(heads=middle_attention['heads'], dtype=middle_attention.get('dtype', jnp.float32), \n",
    "                                    dim_head=middle_dim_out // middle_attention['heads'],\n",
    "                                    use_flash_attention=middle_attention.get(\"flash_attention\", True),\n",
    "                                    use_linear_attention=False,\n",
    "                                    use_projection=middle_attention.get(\"use_projection\", False),\n",
    "                                    use_self_and_cross=False,\n",
    "                                    precision=middle_attention.get(\"precision\", self.precision),\n",
    "                                    name=f\"middle_attention_{j}\")(x)\n",
    "            x = ResidualBlock(\n",
    "                middle_conv_type,\n",
    "                name=f\"middle_res2_{j}\",\n",
    "                features=middle_dim_out,\n",
    "                kernel_init=kernel_init(1.0),\n",
    "                kernel_size=(3, 3),\n",
    "                strides=(1, 1),\n",
    "                activation=self.activation,\n",
    "                norm_groups=self.norm_groups,\n",
    "                dtype=self.dtype,\n",
    "                precision=self.precision\n",
    "            )(x, temb)\n",
    "\n",
    "        # Upscaling Blocks\n",
    "        for i, (dim_out, attention_config) in enumerate(zip(reversed(feature_depths), reversed(attention_configs))):\n",
    "            # print(\"Upscaling\", i, \"features\", dim_out)\n",
    "            for j in range(self.num_res_blocks):\n",
    "                residual = downs.pop()\n",
    "                x = jnp.concatenate([x, residual], axis=-1)\n",
    "                # print(\"concat==> \", i, \"concat\", x.shape)\n",
    "                # kernel_size = (1 + 2 * (j + 1), 1 + 2 * (j + 1))\n",
    "                kernel_size = (3, 3)\n",
    "                x = ResidualBlock(\n",
    "                    up_conv_type,# if j == 0 else \"separable\",\n",
    "                    name=f\"up_{i}_residual_{j}\",\n",
    "                    features=dim_out,\n",
    "                    kernel_init=kernel_init(1.0),\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=(1, 1),\n",
    "                    activation=self.activation,\n",
    "                    norm_groups=self.norm_groups,\n",
    "                    dtype=self.dtype,\n",
    "                    precision=self.precision\n",
    "                )(x, temb)\n",
    "                if attention_config is not None and j == self.num_res_blocks - 1:   # Apply attention only on the last block\n",
    "                    # B, H, W, _ = x.shape\n",
    "                    # if H > TS:\n",
    "                    #     padded_context = jnp.pad(textcontext, ((0, 0), (0, H - TS), (0, 0)), mode='constant', constant_values=0).reshape((B, 1, H, TC))\n",
    "                    # else:\n",
    "                    #     padded_context = None\n",
    "                    x = TransformerBlock(heads=attention_config['heads'], dtype=attention_config.get('dtype', jnp.float32), \n",
    "                                       dim_head=dim_out // attention_config['heads'],\n",
    "                                       use_flash_attention=attention_config.get(\"flash_attention\", True),\n",
    "                                       use_projection=attention_config.get(\"use_projection\", False),\n",
    "                                       use_self_and_cross=attention_config.get(\"use_self_and_cross\", True),\n",
    "                                        precision=attention_config.get(\"precision\", self.precision),\n",
    "                                       name=f\"up_{i}_attention_{j}\")(x, residual)\n",
    "            # print(\"Upscaling \", i, x.shape)\n",
    "            if i != len(feature_depths) - 1:\n",
    "                x = Upsample(\n",
    "                    features=feature_depths[-i],\n",
    "                    scale=2,\n",
    "                    activation=self.activation,\n",
    "                    name=f\"up_{i}_upsample\",\n",
    "                    dtype=self.dtype,\n",
    "                    precision=self.precision\n",
    "                )(x)\n",
    "\n",
    "        # x = nn.GroupNorm(8)(x)\n",
    "        x = ConvLayer(\n",
    "            conv_type,\n",
    "            features=self.feature_depths[0],\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            kernel_init=kernel_init(0.0),\n",
    "            dtype=self.dtype,\n",
    "            precision=self.precision\n",
    "        )(x)\n",
    "    \n",
    "        x = jnp.concatenate([x, downs.pop()], axis=-1)\n",
    "\n",
    "        x = ResidualBlock(\n",
    "            conv_type,\n",
    "            name=\"final_residual\",\n",
    "            features=self.feature_depths[0],\n",
    "            kernel_init=kernel_init(1.0),\n",
    "            kernel_size=(3,3),\n",
    "            strides=(1, 1),\n",
    "            activation=self.activation,\n",
    "            norm_groups=self.norm_groups,\n",
    "            dtype=self.dtype,\n",
    "            precision=self.precision\n",
    "        )(x, temb)\n",
    "\n",
    "        x = nn.RMSNorm()(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        noise_out = ConvLayer(\n",
    "            conv_type,\n",
    "            features=3,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(1, 1),\n",
    "            # activation=jax.nn.mish\n",
    "            kernel_init=kernel_init(0.0),\n",
    "            dtype=self.dtype,\n",
    "            precision=self.precision\n",
    "        )(x)\n",
    "        return noise_out#, attentions\n",
    "\n",
    "# %%\n",
    "import flax.jax_utils\n",
    "import orbax.checkpoint\n",
    "import orbax\n",
    "from typing import Any, Tuple, Mapping,Callable,List,Dict\n",
    "from flax.metrics import tensorboard\n",
    "from functools import partial\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  accuracy: metrics.Accuracy\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "\n",
    "# Define the TrainState \n",
    "class SimpleTrainState(train_state.TrainState):\n",
    "    rngs: jax.random.PRNGKey\n",
    "    metrics: Metrics\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rngs, subkey = jax.random.split(self.rngs)\n",
    "        return self.replace(rngs=rngs), subkey\n",
    "\n",
    "class SimpleTrainer:\n",
    "    state : SimpleTrainState\n",
    "    best_state : SimpleTrainState\n",
    "    best_loss : float\n",
    "    model : nn.Module\n",
    "    ema_decay:float = 0.999\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model:nn.Module, \n",
    "                 input_shapes:Dict[str, Tuple[int]],\n",
    "                 optimizer: optax.GradientTransformation,\n",
    "                 rngs:jax.random.PRNGKey,\n",
    "                 train_state:SimpleTrainState=None,\n",
    "                 name:str=\"Simple\",\n",
    "                 load_from_checkpoint:bool=False,\n",
    "                 checkpoint_suffix:str=\"\",\n",
    "                 loss_fn=optax.l2_loss,\n",
    "                 param_transforms:Callable=None,\n",
    "                 wandb_config:Dict[str, Any]=None\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.loss_fn = loss_fn\n",
    "        self.input_shapes = input_shapes\n",
    "        \n",
    "        if wandb_config is not None:\n",
    "            run = wandb.init(**wandb_config)\n",
    "            self.wandb = run\n",
    "\n",
    "        checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=4, create=True)\n",
    "        self.checkpointer = orbax.checkpoint.CheckpointManager(self.checkpoint_path() + checkpoint_suffix, checkpointer, options)\n",
    "\n",
    "        if load_from_checkpoint:\n",
    "            latest_epoch, old_state, old_best_state = self.load()\n",
    "        else:\n",
    "            latest_epoch, old_state, old_best_state = 0, None, None\n",
    "            \n",
    "        self.latest_epoch = latest_epoch\n",
    "\n",
    "        if train_state == None:\n",
    "            self.init_state(optimizer, rngs, existing_state=old_state, existing_best_state=old_best_state, model=model, param_transforms=param_transforms)\n",
    "        else:\n",
    "            self.state = train_state\n",
    "            self.best_state = train_state\n",
    "            self.best_loss = 1e9\n",
    "    \n",
    "    def get_input_ones(self):\n",
    "        return {k:jnp.ones((1, *v)) for k,v in self.input_shapes.items()}\n",
    "\n",
    "    def init_state(self,\n",
    "                   optimizer: optax.GradientTransformation, \n",
    "                   rngs:jax.random.PRNGKey,\n",
    "                   existing_state:dict=None,\n",
    "                   existing_best_state:dict=None,\n",
    "                   model:nn.Module=None,\n",
    "                   param_transforms:Callable=None\n",
    "                   ):\n",
    "        @partial(jax.pmap, axis_name=\"device\")\n",
    "        def init_fn(rngs):\n",
    "            rngs, subkey = jax.random.split(rngs)\n",
    "\n",
    "            if existing_state == None:\n",
    "                input_vars = self.get_input_ones()\n",
    "                params = model.init(subkey, **input_vars)\n",
    "\n",
    "            # if param_transforms is not None:\n",
    "            #     params = param_transforms(params)\n",
    "                \n",
    "            state = SimpleTrainState.create(\n",
    "                apply_fn=model.apply,\n",
    "                params=params,\n",
    "                tx=optimizer,\n",
    "                rngs=rngs,\n",
    "                metrics=Metrics.empty()\n",
    "            )\n",
    "            return state\n",
    "        self.state = init_fn(jax.device_put_replicated(rngs, jax.devices()))\n",
    "        self.best_loss = 1e9\n",
    "        if existing_best_state is not None:\n",
    "            self.best_state = self.state.replace(params=existing_best_state['params'], ema_params=existing_best_state['ema_params'])\n",
    "        else:\n",
    "            self.best_state = self.state\n",
    "            \n",
    "    def get_state(self):\n",
    "        return flax.jax_utils.unreplicate(self.state)\n",
    "\n",
    "    def get_best_state(self):\n",
    "        return flax.jax_utils.unreplicate(self.best_state)\n",
    "\n",
    "    def checkpoint_path(self):\n",
    "        experiment_name = self.name\n",
    "        path = os.path.join(os.path.abspath('./checkpoints'), experiment_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "    \n",
    "    def tensorboard_path(self):\n",
    "        experiment_name = self.name\n",
    "        path = os.path.join(os.path.abspath('./tensorboard'), experiment_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def load(self):\n",
    "        epoch = self.checkpointer.latest_step()\n",
    "        print(\"Loading model from checkpoint\", epoch)\n",
    "        ckpt = self.checkpointer.restore(epoch)\n",
    "        state = ckpt['state']\n",
    "        best_state = ckpt['best_state']\n",
    "        # Convert the state to a TrainState\n",
    "        self.best_loss = ckpt['best_loss']\n",
    "        print(f\"Loaded model from checkpoint at epoch {epoch}\", ckpt['best_loss'])\n",
    "        return epoch, state, best_state\n",
    "\n",
    "    def save(self, epoch=0):\n",
    "        print(f\"Saving model at epoch {epoch}\")\n",
    "        ckpt = {\n",
    "            # 'model': self.model,\n",
    "            'state': self.get_state(),\n",
    "            'best_state': self.get_best_state(),\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        try:\n",
    "            save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "            self.checkpointer.save(epoch, ckpt, save_kwargs={'save_args': save_args}, force=True)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(\"Error saving checkpoint\", e)\n",
    "\n",
    "    def _define_train_step(self, **kwargs):\n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "        \n",
    "        @partial(jax.pmap, axis_name=\"device\")\n",
    "        def train_step(state:SimpleTrainState, batch):\n",
    "            \"\"\"Train for a single step.\"\"\"\n",
    "            images = batch['image']\n",
    "            labels= batch['label']\n",
    "            \n",
    "            def model_loss(params):\n",
    "                preds = model.apply(params, images)\n",
    "                expected_output = labels\n",
    "                nloss = loss_fn(preds, expected_output)\n",
    "                loss = jnp.mean(nloss)\n",
    "                return loss\n",
    "            loss, grads = jax.value_and_grad(model_loss)(state.params)\n",
    "            grads = jax.lax.pmean(grads, \"device\")\n",
    "            state = state.apply_gradients(grads=grads) \n",
    "            return state, loss\n",
    "        return train_step\n",
    "    \n",
    "    def _define_compute_metrics(self):\n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "        \n",
    "        @jax.jit\n",
    "        def compute_metrics(state:SimpleTrainState, batch):\n",
    "            preds = model.apply(state.params, batch['image'])\n",
    "            expected_output = batch['label']\n",
    "            loss = jnp.mean(loss_fn(preds, expected_output))\n",
    "            metric_updates = state.metrics.single_from_model_output(loss=loss, logits=preds, labels=expected_output)\n",
    "            metrics = state.metrics.merge(metric_updates)\n",
    "            state = state.replace(metrics=metrics)\n",
    "            return state\n",
    "        return compute_metrics\n",
    "\n",
    "    def summary(self):\n",
    "        input_vars = self.get_input_ones()\n",
    "        print(self.model.tabulate(jax.random.key(0), **input_vars, console_kwargs={\"width\": 200, \"force_jupyter\":True, }))\n",
    "    \n",
    "    def config(self):\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"state\": self.state,\n",
    "            \"name\": self.name,\n",
    "            \"input_shapes\": self.input_shapes\n",
    "        }\n",
    "        \n",
    "    def init_tensorboard(self, batch_size, steps_per_epoch, epochs):\n",
    "        summary_writer = tensorboard.SummaryWriter(self.tensorboard_path())\n",
    "        summary_writer.hparams({\n",
    "            **self.config(),\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size\n",
    "        })\n",
    "        return summary_writer\n",
    "        \n",
    "    def fit(self, data, steps_per_epoch, epochs, train_step_args={}):\n",
    "        train_ds = iter(data['train']())\n",
    "        if 'test' in data:\n",
    "            test_ds = data['test']\n",
    "        else:\n",
    "            test_ds = None\n",
    "        train_step = self._define_train_step(**train_step_args)\n",
    "        compute_metrics = self._define_compute_metrics()\n",
    "        state = self.state\n",
    "        device_count = jax.device_count()\n",
    "        # train_ds = flax.jax_utils.prefetch_to_device(train_ds, jax.devices())\n",
    "        \n",
    "        summary_writer = self.init_tensorboard(data['batch_size'], steps_per_epoch, epochs)\n",
    "        \n",
    "        while self.latest_epoch <= epochs:\n",
    "            self.latest_epoch += 1\n",
    "            current_epoch = self.latest_epoch\n",
    "            print(f\"\\nEpoch {current_epoch}/{epochs}\")\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            with tqdm.tqdm(total=steps_per_epoch, desc=f'\\t\\tEpoch {current_epoch}', ncols=100, unit='step') as pbar:\n",
    "                for i in range(steps_per_epoch):\n",
    "                    batch = next(train_ds)\n",
    "                    batch = jax.tree.map(lambda x: x.reshape((device_count, -1, *x.shape[1:])), batch)\n",
    "                    # print(batch['image'].shape)\n",
    "                    state, loss = train_step(state, batch)\n",
    "                    loss = jnp.mean(loss)\n",
    "                    # print(\"==>\", loss)\n",
    "                    epoch_loss += loss\n",
    "                    if i % 100 == 0:\n",
    "                        pbar.set_postfix(loss=f'{loss:.4f}')\n",
    "                        pbar.update(100)\n",
    "                        current_step = current_epoch*steps_per_epoch + i\n",
    "                        summary_writer.scalar('Train Loss', loss, step=current_step)\n",
    "                        if self.wandb is not None:\n",
    "                            self.wandb.log({\"train/loss\": loss})\n",
    "            \n",
    "            print(f\"\\n\\tEpoch done\")\n",
    "            end_time = time.time()\n",
    "            self.state = state\n",
    "            total_time = end_time - start_time\n",
    "            avg_time_per_step = total_time / steps_per_epoch\n",
    "            avg_loss = epoch_loss / steps_per_epoch\n",
    "            if avg_loss < self.best_loss:\n",
    "                self.best_loss = avg_loss\n",
    "                self.best_state = state\n",
    "                self.save(current_epoch)\n",
    "            \n",
    "            # Compute Metrics\n",
    "            metrics_str = ''\n",
    "                    \n",
    "            print(f\"\\n\\tEpoch {current_epoch} completed. Avg Loss: {avg_loss}, Time: {total_time:.2f}s, Best Loss: {self.best_loss} {metrics_str}\")\n",
    "            \n",
    "        self.save(epochs)\n",
    "        return self.state\n",
    "\n",
    "# Define the TrainState with EMA parameters\n",
    "class TrainState(SimpleTrainState):\n",
    "    rngs: jax.random.PRNGKey\n",
    "    ema_params: dict\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rngs, subkey = jax.random.split(self.rngs)\n",
    "        return self.replace(rngs=rngs), subkey\n",
    "\n",
    "    def apply_ema(self, decay: float=0.999):\n",
    "        new_ema_params = jax.tree_util.tree_map(\n",
    "            lambda ema, param: decay * ema + (1 - decay) * param,\n",
    "            self.ema_params,\n",
    "            self.params,\n",
    "        )\n",
    "        return self.replace(ema_params=new_ema_params)\n",
    "\n",
    "class DiffusionTrainer(SimpleTrainer):\n",
    "    noise_schedule : NoiseScheduler\n",
    "    model_output_transform:DiffusionPredictionTransform\n",
    "    ema_decay:float = 0.999\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model:nn.Module, \n",
    "                 input_shapes:Dict[str, Tuple[int]],\n",
    "                 optimizer: optax.GradientTransformation,\n",
    "                 noise_schedule:NoiseScheduler,\n",
    "                 rngs:jax.random.PRNGKey,\n",
    "                 unconditional_prob:float=0.2,\n",
    "                 name:str=\"Diffusion\",\n",
    "                 model_output_transform:DiffusionPredictionTransform=EpsilonPredictionTransform(),\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            input_shapes=input_shapes,\n",
    "            optimizer=optimizer,\n",
    "            rngs=rngs,\n",
    "            name=name,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.model_output_transform = model_output_transform\n",
    "        self.unconditional_prob = unconditional_prob\n",
    "\n",
    "    def init_state(self, \n",
    "                   optimizer: optax.GradientTransformation, \n",
    "                   rngs:jax.random.PRNGKey,\n",
    "                   existing_state:dict=None,\n",
    "                   existing_best_state:dict=None,\n",
    "                   model:nn.Module=None,\n",
    "                   param_transforms:Callable=None,\n",
    "                   ):\n",
    "        # @partial(jax.pmap, axis_name=\"device\")\n",
    "        def init_fn(rngs):\n",
    "            rngs, subkey = jax.random.split(rngs)\n",
    "\n",
    "            if existing_state == None:\n",
    "                input_vars = self.get_input_ones()\n",
    "                params = model.init(subkey, **input_vars)\n",
    "                new_state = {\"params\":params, \"ema_params\":params}\n",
    "            else:\n",
    "                new_state = existing_state\n",
    "\n",
    "            if param_transforms is not None:\n",
    "                params = param_transforms(params)\n",
    "            \n",
    "            state = TrainState.create(\n",
    "                apply_fn=model.apply,\n",
    "                params=new_state['params'],\n",
    "                ema_params=new_state['ema_params'],\n",
    "                tx=optimizer,\n",
    "                rngs=rngs,\n",
    "                metrics=Metrics.empty()\n",
    "            )\n",
    "            return state\n",
    "            \n",
    "        self.best_loss = 1e9\n",
    "        # self.state = init_fn(jax.device_put_replicated(rngs, jax.devices()))\n",
    "        state = init_fn(rngs)\n",
    "        if existing_best_state is not None:\n",
    "            best_state = state.replace(params=existing_best_state['params'], ema_params=existing_best_state['ema_params'])\n",
    "        else:\n",
    "            best_state = state\n",
    "            \n",
    "        self.state = flax.jax_utils.replicate(state, jax.devices())\n",
    "        self.best_state = flax.jax_utils.replicate(best_state, jax.devices())\n",
    "\n",
    "    def _define_train_step(self, batch_size, null_labels_seq, text_embedder):\n",
    "        noise_schedule = self.noise_schedule\n",
    "        model = self.model\n",
    "        model_output_transform = self.model_output_transform\n",
    "        loss_fn = self.loss_fn\n",
    "        unconditional_prob = self.unconditional_prob\n",
    "        \n",
    "        # Determine the number of unconditional samples\n",
    "        num_unconditional = int(batch_size * unconditional_prob)\n",
    "        \n",
    "        nS, nC = null_labels_seq.shape\n",
    "        null_labels_seq = jnp.broadcast_to(null_labels_seq, (batch_size, nS, nC))\n",
    "        \n",
    "        # @jax.jit\n",
    "        @partial(jax.pmap, axis_name=\"device\")\n",
    "        def train_step(state:TrainState, batch):\n",
    "            \"\"\"Train for a single step.\"\"\"\n",
    "            images = batch['image']\n",
    "            # normalize image\n",
    "            images = (images - 127.5) / 127.5\n",
    "            \n",
    "            output = text_embedder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            # output = infer(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            \n",
    "            label_seq = output.last_hidden_state\n",
    "            \n",
    "            # Generate random probabilities to decide how much of this batch will be unconditional\n",
    "            \n",
    "            label_seq = jnp.concat([null_labels_seq[:num_unconditional], label_seq[num_unconditional:]], axis=0)\n",
    "\n",
    "            noise_level, state = noise_schedule.generate_timesteps(images.shape[0], state)\n",
    "            state, rngs = state.get_random_key()\n",
    "            noise:jax.Array = jax.random.normal(rngs, shape=images.shape)\n",
    "            rates = noise_schedule.get_rates(noise_level)\n",
    "            noisy_images, c_in, expected_output = model_output_transform.forward_diffusion(images, noise, rates)\n",
    "            def model_loss(params):\n",
    "                preds = model.apply(params, *noise_schedule.transform_inputs(noisy_images*c_in, noise_level), label_seq)\n",
    "                preds = model_output_transform.pred_transform(noisy_images, preds, rates)\n",
    "                nloss = loss_fn(preds, expected_output)\n",
    "                # nloss = jnp.mean(nloss, axis=1)\n",
    "                nloss *= noise_schedule.get_weights(noise_level)\n",
    "                nloss = jnp.mean(nloss)\n",
    "                loss = nloss\n",
    "                return loss\n",
    "            loss, grads = jax.value_and_grad(model_loss)(state.params)\n",
    "            grads = jax.lax.pmean(grads, \"device\")\n",
    "            state = state.apply_gradients(grads=grads) \n",
    "            state = state.apply_ema(self.ema_decay)\n",
    "            return state, loss\n",
    "        return train_step\n",
    "    \n",
    "    def _define_compute_metrics(self):\n",
    "        @jax.jit\n",
    "        def compute_metrics(state:TrainState, expected, pred):\n",
    "            loss = jnp.mean(jnp.square(pred - expected))\n",
    "            metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "            metrics = state.metrics.merge(metric_updates)\n",
    "            state = state.replace(metrics=metrics)\n",
    "            return state\n",
    "        return compute_metrics\n",
    "\n",
    "    def fit(self, data, steps_per_epoch, epochs):\n",
    "        null_labels_full = data['null_labels_full']\n",
    "        batch_size = data['batch_size']\n",
    "        text_embedder = data['model']\n",
    "        super().fit(data, steps_per_epoch, epochs, {\"batch_size\":batch_size, \"null_labels_seq\":null_labels_full, \"text_embedder\":text_embedder})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE_MAP = {\n",
    "    'bfloat16': jnp.bfloat16,\n",
    "    'float32': jnp.float32\n",
    "}\n",
    "\n",
    "PRECISION_MAP = {\n",
    "    'high': jax.lax.Precision.HIGH,\n",
    "    'default': jax.lax.Precision.DEFAULT,\n",
    "    'highes': jax.lax.Precision.HIGHEST\n",
    "}\n",
    "\n",
    "ACTIVATION_MAP = {\n",
    "    'swish': jax.nn.swish,\n",
    "    'mish': jax.nn.mish,\n",
    "}\n",
    "\n",
    "DTYPE = DTYPE_MAP[\"bfloat16\"]\n",
    "PRECISION = PRECISION_MAP[\"high\"]\n",
    "\n",
    "GRAIN_WORKER_COUNT = 16\n",
    "GRAIN_READ_THREAD_COUNT = 64\n",
    "GRAIN_READ_BUFFER_SIZE = 50\n",
    "GRAIN_WORKER_BUFFER_SIZE = 20\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "dataset_name = 'cc12m'\n",
    "datalen = len(datasetMap[dataset_name]['source']())\n",
    "batches = datalen // BATCH_SIZE\n",
    "\n",
    "# Define the configuration using the command-line arguments\n",
    "attention_configs = [\n",
    "    None,\n",
    "]\n",
    "\n",
    "attention_configs += [\n",
    "    {\"heads\": 8, \"dtype\": DTYPE, \"flash_attention\": False, \"use_projection\": False, \"use_self_and_cross\": False},\n",
    "] * (2)\n",
    "\n",
    "attention_configs += [\n",
    "    {\"heads\": 8, \"dtype\": DTYPE, \"flash_attention\": False, \"use_projection\": False, \"use_self_and_cross\": False},\n",
    "]\n",
    "\n",
    "CONFIG = {\n",
    "    \"model\": {\n",
    "        \"emb_features\": 256,\n",
    "        \"feature_depths\": [64, 128, 256, 512],\n",
    "        \"attention_configs\": attention_configs,\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"num_middle_res_blocks\": 1,\n",
    "        \"dtype\": DTYPE,\n",
    "        \"precision\": PRECISION,\n",
    "        \"activation\": ACTIVATION_MAP[\"swish\"],\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"name\": dataset_name,\n",
    "        \"length\": datalen,\n",
    "        \"batches\": datalen // BATCH_SIZE,\n",
    "    },\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"input_shapes\": {\n",
    "        \"x\": (128, 128, 3),\n",
    "        \"temb\": (),\n",
    "        \"textcontext\": (77, 768)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_Name: Diffusion_SDE_VE_TEXT_2024-08-01_20:56:25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f522aa7m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Diffusion_SDE_VE_TEXT_2024-08-01_20:54:34</strong> at: <a href='https://wandb.ai/ashishkumar4/flaxdiff/runs/f522aa7m' target=\"_blank\">https://wandb.ai/ashishkumar4/flaxdiff/runs/f522aa7m</a><br/> View project at: <a href='https://wandb.ai/ashishkumar4/flaxdiff' target=\"_blank\">https://wandb.ai/ashishkumar4/flaxdiff</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240801_205434-f522aa7m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f522aa7m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mrwhite0racle/research/FlaxDiff/wandb/run-20240801_205625-xunupj6f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ashishkumar4/flaxdiff/runs/xunupj6f' target=\"_blank\">Diffusion_SDE_VE_TEXT_2024-08-01_20:56:25</a></strong> to <a href='https://wandb.ai/ashishkumar4/flaxdiff' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ashishkumar4/flaxdiff' target=\"_blank\">https://wandb.ai/ashishkumar4/flaxdiff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ashishkumar4/flaxdiff/runs/xunupj6f' target=\"_blank\">https://wandb.ai/ashishkumar4/flaxdiff/runs/xunupj6f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cc12m dataset with 1000 samples\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEpoch 1: 100%|| 1000/1000 [02:46<00:00,  6.01step/s, loss=0.0968]\n",
      "WARNING:absl:SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEpoch done\n",
      "Saving model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `aggregate` option is deprecated and will be ignored.\n",
      "WARNING:absl:The `aggregate` option is deprecated and will be ignored.\n",
      "WARNING:absl:The `aggregate` option is deprecated and will be ignored.\n",
      "WARNING:absl:The `aggregate` option is deprecated and will be ignored.\n",
      "WARNING:absl:The `aggregate` option is deprecated and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEpoch 1 completed. Avg Loss: 0.18282465636730194, Time: 166.29s, Best Loss: 0.18282465636730194 \n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEpoch 2: 100%|| 1000/1000 [01:14<00:00, 13.48step/s, loss=0.0877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tEpoch done\n",
      "Saving model at epoch 2\n",
      "\n",
      "\tEpoch 2 completed. Avg Loss: 0.08512131124734879, Time: 74.18s, Best Loss: 0.08512131124734879 \n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tEpoch 3:  20%|                           | 200/1000 [00:13<00:52, 15.28step/s, loss=0.0988]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m jax\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mstart_server(\u001b[38;5;241m6009\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 902\u001b[0m, in \u001b[0;36mDiffusionTrainer.fit\u001b[0;34m(self, data, steps_per_epoch, epochs)\u001b[0m\n\u001b[1;32m    900\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    901\u001b[0m text_embedder \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 902\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnull_labels_seq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mnull_labels_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_embedder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtext_embedder\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 719\u001b[0m, in \u001b[0;36mSimpleTrainer.fit\u001b[0;34m(self, data, steps_per_epoch, epochs, train_step_args)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# print(batch['image'].shape)\u001b[39;00m\n\u001b[1;32m    718\u001b[0m state, loss \u001b[38;5;241m=\u001b[39m train_step(state, batch)\n\u001b[0;32m--> 719\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# print(\"==>\", loss)\u001b[39;00m\n\u001b[1;32m    721\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/reductions.py:739\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(a: ArrayLike, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dtype: DTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    682\u001b[0m          out: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, keepdims: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    683\u001b[0m          where: ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    684\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the mean of array elements along a given axis.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m  JAX implementation of :func:`numpy.mean`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m           [6. ]], dtype=float32)\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ensure_optional_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/pjit.py:347\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.<lambda>\u001b[0;34m(x, sharding)\u001b[0m\n\u001b[1;32m    336\u001b[0m   maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    337\u001b[0m       executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr\u001b[38;5;241m.\u001b[39meffects,\n\u001b[1;32m    338\u001b[0m       jaxpr\u001b[38;5;241m.\u001b[39mconsts, jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes,\n\u001b[1;32m    339\u001b[0m       pgle_profiler)\n\u001b[1;32m    341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n\u001b[1;32m    343\u001b[0m cpp_pjit_f \u001b[38;5;241m=\u001b[39m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(\n\u001b[1;32m    344\u001b[0m   fun_name(fun),\n\u001b[1;32m    345\u001b[0m   fun, cache_miss, jit_info\u001b[38;5;241m.\u001b[39mstatic_argnums, jit_info\u001b[38;5;241m.\u001b[39mstatic_argnames,\n\u001b[1;32m    346\u001b[0m   jit_info\u001b[38;5;241m.\u001b[39mdonate_argnums, tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[0;32m--> 347\u001b[0m   \u001b[38;5;28;01mlambda\u001b[39;00m x, sharding: \u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshard_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    348\u001b[0m   _get_cpp_global_cache(jit_info\u001b[38;5;241m.\u001b[39mhas_explicit_sharding))\n\u001b[1;32m    350\u001b[0m cpp_pjitted_f \u001b[38;5;241m=\u001b[39m wraps(fun)(cpp_pjit_f)\n\u001b[1;32m    351\u001b[0m cpp_pjitted_f\u001b[38;5;241m.\u001b[39m_fun \u001b[38;5;241m=\u001b[39m fun\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:116\u001b[0m, in \u001b[0;36mshard_args\u001b[0;34m(shardings, args, canonicalize)\u001b[0m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m canonicalize:\n\u001b[1;32m    115\u001b[0m     arg \u001b[38;5;241m=\u001b[39m xla\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(arg)\n\u001b[0;32m--> 116\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshard_arg_handlers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43marg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshardings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# type(arg) -> (indices, args, shardings)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m batches \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: ([], [], []))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:1140\u001b[0m, in \u001b[0;36m_array_shard_arg\u001b[0;34m(xs, shardings)\u001b[0m\n\u001b[1;32m   1137\u001b[0m       results\u001b[38;5;241m.\u001b[39mappend(shard_device_array(x, devices, indices, sharding))\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1139\u001b[0m       results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m-> 1140\u001b[0m           \u001b[43mshard_sharded_device_array_slow_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1142\u001b[0m copy_outs \u001b[38;5;241m=\u001b[39m xc\u001b[38;5;241m.\u001b[39mbatched_copy_array_to_devices_with_sharding(\n\u001b[1;32m   1143\u001b[0m     batch_xs, batch_devs, batch_shardings)\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, copy_out \u001b[38;5;129;01min\u001b[39;00m safe_zip(batch_indices, copy_outs):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:1091\u001b[0m, in \u001b[0;36mshard_sharded_device_array_slow_path\u001b[0;34m(x, devices, indices, sharding)\u001b[0m\n\u001b[1;32m   1087\u001b[0m candidates_list \u001b[38;5;241m=\u001b[39m candidates[hashed_index(idx)]\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates_list:\n\u001b[1;32m   1089\u001b[0m   \u001b[38;5;66;03m# This array isn't sharded correctly. Reshard it via host roundtrip.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m   \u001b[38;5;66;03m# TODO(skye): more efficient reshard?\u001b[39;00m\n\u001b[0;32m-> 1091\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mshard_args([sharding], [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m], canonicalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# Try to find a candidate buffer already on the correct device,\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# otherwise copy one of them.\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m buf \u001b[38;5;129;01min\u001b[39;00m candidates_list:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:336\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    335\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:634\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m npy_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ind \u001b[38;5;129;01min\u001b[39;00m _cached_index_calc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 634\u001b[0m   npy_value[ind] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrays\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m npy_value\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cosine_schedule = CosineNoiseSchedule(1000, beta_end=1)\n",
    "karas_ve_schedule = KarrasVENoiseScheduler(1, sigma_max=80, rho=7, sigma_data=0.5)\n",
    "edm_schedule = EDMNoiseScheduler(1, sigma_max=80, rho=7, sigma_data=0.5)\n",
    "\n",
    "experiment_name = \"{name}_{date}\".format(\n",
    "    name=\"Diffusion_SDE_VE_TEXT\", date=datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    ")\n",
    "print(\"Experiment_Name:\", experiment_name)\n",
    "\n",
    "unet = Unet(**CONFIG['model'])\n",
    "\n",
    "learning_rate = CONFIG['learning_rate']\n",
    "solver = optax.adam(learning_rate)\n",
    "# solver = optax.adamw(2e-6)\n",
    "\n",
    "trainer = DiffusionTrainer(unet, optimizer=solver, \n",
    "                           input_shapes=CONFIG['input_shapes'], \n",
    "                           noise_schedule=edm_schedule,\n",
    "                           rngs=jax.random.PRNGKey(4), \n",
    "                           name=experiment_name,\n",
    "                           model_output_transform=KarrasPredictionTransform(sigma_data=edm_schedule.sigma_data),\n",
    "                        #    train_state=trainer.best_state,\n",
    "                        #    loss_fn=lambda x, y: jnp.abs(x - y),\n",
    "                            # param_transforms=params_transform,\n",
    "                        #    load_from_checkpoint=True,\n",
    "                           wandb_config={\n",
    "                                 \"project\": \"flaxdiff\",\n",
    "                                 \"config\": CONFIG,\n",
    "                                 \"name\": experiment_name,\n",
    "                               },\n",
    "                           )\n",
    "\n",
    "\n",
    "# %%\n",
    "# print(trainer.summary())\n",
    "\n",
    "# %%\n",
    "data = get_dataset_grain(CONFIG['dataset']['name'], \n",
    "                         batch_size=BATCH_SIZE, image_scale=IMAGE_SIZE,\n",
    "                         grain_worker_count=GRAIN_WORKER_COUNT, grain_read_thread_count=GRAIN_READ_THREAD_COUNT,\n",
    "                        grain_read_buffer_size=GRAIN_READ_BUFFER_SIZE, grain_worker_buffer_size=GRAIN_WORKER_BUFFER_SIZE,\n",
    "                         )\n",
    "\n",
    "batches = 1000\n",
    "print(f\"Training on {CONFIG['dataset']['name']} dataset with {batches} samples\")\n",
    "jax.profiler.start_server(6009)\n",
    "final_state = trainer.fit(data, batches, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
