{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple, Mapping, Callable, List, Dict\n",
    "from functools import partial\n",
    "from flax.metrics import tensorboard\n",
    "import orbax\n",
    "import orbax.checkpoint\n",
    "import flax.jax_utils\n",
    "from flaxdiff.models.simple_unet import Unet\n",
    "import jax.experimental.pallas.ops.tpu.flash_attention\n",
    "from flaxdiff.predictors import VPredictionTransform, EpsilonPredictionTransform, DiffusionPredictionTransform, DirectPredictionTransform, KarrasPredictionTransform\n",
    "from flaxdiff.schedulers import CosineNoiseSchedule, NoiseScheduler, GeneralizedNoiseScheduler, KarrasVENoiseScheduler, EDMNoiseScheduler\n",
    "import struct as st\n",
    "import flax\n",
    "import tqdm\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from typing import Dict, Callable, Sequence, Any, Union\n",
    "from dataclasses import field\n",
    "import jax.numpy as jnp\n",
    "import grain.python as pygrain\n",
    "import numpy as np\n",
    "import augmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "import optax\n",
    "from flax import struct                # Flax dataclasses\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from flax.training import orbax_utils\n",
    "import functools\n",
    "\n",
    "import json\n",
    "# For CLIP\n",
    "from transformers import AutoTokenizer, FlaxCLIPTextModel, CLIPTextModel\n",
    "import wandb\n",
    "import cv2\n",
    "import argparse\n",
    "\n",
    "import resource\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.experimental.shard_map import shard_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeImage = lambda x: jax.nn.standardize(x, mean=[127.5], std=[127.5])\n",
    "denormalizeImage = lambda x: (x + 1.0) * 127.5\n",
    "\n",
    "\n",
    "def plotImages(imgs, fig_size=(8, 8), dpi=100):\n",
    "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
    "    imglen = imgs.shape[0]\n",
    "    for i in range(imglen):\n",
    "        plt.subplot(fig_size[0], fig_size[1], i + 1)\n",
    "        plt.imshow(jnp.astype(denormalizeImage(imgs[i, :, :, :]), jnp.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovState(struct.PyTreeNode):\n",
    "    pass\n",
    "\n",
    "\n",
    "class RandomMarkovState(MarkovState):\n",
    "    rng: jax.random.PRNGKey\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rng, subkey = jax.random.split(self.rng)\n",
    "        return RandomMarkovState(rng), subkey\n",
    "    \n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output('loss')\n",
    "\n",
    "# Define the TrainState\n",
    "class SimpleTrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "\n",
    "class SimpleTrainer:\n",
    "    state: SimpleTrainState\n",
    "    best_state: SimpleTrainState\n",
    "    best_loss: float\n",
    "    model: nn.Module\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 input_shapes: Dict[str, Tuple[int]],\n",
    "                 optimizer: optax.GradientTransformation,\n",
    "                 rngs: jax.random.PRNGKey,\n",
    "                 train_state: SimpleTrainState = None,\n",
    "                 name: str = \"Simple\",\n",
    "                 load_from_checkpoint: bool = False,\n",
    "                 checkpoint_suffix: str = \"\",\n",
    "                 loss_fn=optax.l2_loss,\n",
    "                 param_transforms: Callable = None,\n",
    "                 wandb_config: Dict[str, Any] = None,\n",
    "                 distributed_training: bool = None,\n",
    "                 ):\n",
    "        if distributed_training is None or distributed_training is True:\n",
    "            # Auto-detect if we are running on multiple devices\n",
    "            distributed_training = jax.device_count() > 1\n",
    "            self.mesh = jax.sharding.Mesh(jax.devices(), 'data')\n",
    "\n",
    "        self.distributed_training = distributed_training\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.loss_fn = loss_fn\n",
    "        self.input_shapes = input_shapes\n",
    "\n",
    "        if wandb_config is not None and jax.process_index() == 0:\n",
    "            run = wandb.init(**wandb_config)\n",
    "            self.wandb = run\n",
    "            # define our custom x axis metric\n",
    "            self.wandb.define_metric(\"train/step\")\n",
    "            self.wandb.define_metric(\"train/epoch\")\n",
    "            \n",
    "            self.wandb.define_metric(\"train/loss\", step_metric=\"train/step\")\n",
    "            \n",
    "            self.wandb.define_metric(\"train/epoch_time\", step_metric=\"train/epoch\")\n",
    "            self.wandb.define_metric(\"train/avg_time_per_step\", step_metric=\"train/epoch\")\n",
    "            self.wandb.define_metric(\"train/avg_loss\", step_metric=\"train/epoch\")\n",
    "            self.wandb.define_metric(\"train/best_loss\", step_metric=\"train/epoch\")\n",
    "\n",
    "        checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        options = orbax.checkpoint.CheckpointManagerOptions(\n",
    "            max_to_keep=4, create=True)\n",
    "        self.checkpointer = orbax.checkpoint.CheckpointManager(\n",
    "            self.checkpoint_path() + checkpoint_suffix, checkpointer, options)\n",
    "\n",
    "        if load_from_checkpoint:\n",
    "            latest_epoch, old_state, old_best_state, rngstate = self.load()\n",
    "        else:\n",
    "            latest_epoch, old_state, old_best_state, rngstate = 0, None, None, None\n",
    "\n",
    "        self.latest_epoch = latest_epoch\n",
    "        \n",
    "        if rngstate:\n",
    "            self.rngstate = rngstate\n",
    "        else:\n",
    "            rngs, subkey = jax.random.split(rngs)\n",
    "            self.rngstate = RandomMarkovState(subkey)\n",
    "\n",
    "        if train_state == None:\n",
    "            state, best_state = self.generate_states(\n",
    "                optimizer, rngs, old_state, old_best_state, model, param_transforms\n",
    "            )\n",
    "            self.init_state(state, best_state)\n",
    "        else:\n",
    "            self.state = train_state\n",
    "            self.best_state = train_state\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "    def get_input_ones(self):\n",
    "        return {k: jnp.ones((1, *v)) for k, v in self.input_shapes.items()}\n",
    "\n",
    "    def generate_states(\n",
    "        self,\n",
    "        optimizer: optax.GradientTransformation,\n",
    "        rngs: jax.random.PRNGKey,\n",
    "        existing_state: dict = None,\n",
    "        existing_best_state: dict = None,\n",
    "        model: nn.Module = None,\n",
    "        param_transforms: Callable = None\n",
    "    ) -> Tuple[SimpleTrainState, SimpleTrainState]:\n",
    "        print(\"Generating states for SimpleTrainer\")\n",
    "        rngs, subkey = jax.random.split(rngs)\n",
    "\n",
    "        if existing_state == None:\n",
    "            input_vars = self.get_input_ones()\n",
    "            params = model.init(subkey, **input_vars)\n",
    "        else:\n",
    "            params = existing_state['params']\n",
    "\n",
    "        state = SimpleTrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=params,\n",
    "            tx=optimizer,\n",
    "            metrics=Metrics.empty()\n",
    "        )\n",
    "        if existing_best_state is not None:\n",
    "            best_state = state.replace(\n",
    "                params=existing_best_state['params'])\n",
    "        else:\n",
    "            best_state = state\n",
    "            \n",
    "        return state, best_state\n",
    "\n",
    "    def init_state(\n",
    "        self,\n",
    "        state: SimpleTrainState,\n",
    "        best_state: SimpleTrainState,\n",
    "    ):\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "        if self.distributed_training:\n",
    "            devices = jax.local_devices()\n",
    "            if len(devices) > 1:\n",
    "                print(\"Replicating state across devices \", devices)\n",
    "                state = flax.jax_utils.replicate(state, devices)\n",
    "                best_state = flax.jax_utils.replicate(best_state, devices)\n",
    "                self.rngstate = flax.jax_utils.replicate(self.rngstate, devices)\n",
    "            else:\n",
    "                print(\"Not replicating any state, Only single device connected to the process\")\n",
    "\n",
    "        self.state = state\n",
    "        self.best_state = best_state\n",
    "\n",
    "    def get_state(self):\n",
    "        if self.distributed_training and jax.process_index() == 0:\n",
    "            return flax.jax_utils.unreplicate(self.state)\n",
    "        else:\n",
    "            return self.state\n",
    "\n",
    "    def get_best_state(self):\n",
    "        if self.distributed_training and jax.process_index() == 0:\n",
    "            return flax.jax_utils.unreplicate(self.best_state)\n",
    "        else:\n",
    "            return self.best_state\n",
    "        \n",
    "    def get_rngstate(self):\n",
    "        if self.distributed_training and jax.process_index() == 0:\n",
    "            return flax.jax_utils.unreplicate(self.rngstate)\n",
    "        else:\n",
    "            return self.rngstate\n",
    "\n",
    "    def checkpoint_path(self):\n",
    "        experiment_name = self.name\n",
    "        path = os.path.join(os.path.abspath('./checkpoints'), experiment_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def tensorboard_path(self):\n",
    "        experiment_name = self.name\n",
    "        path = os.path.join(os.path.abspath('./tensorboard'), experiment_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def load(self):\n",
    "        epoch = self.checkpointer.latest_step()\n",
    "        print(\"Loading model from checkpoint\", epoch)\n",
    "        ckpt = self.checkpointer.restore(epoch)\n",
    "        state = ckpt['state']\n",
    "        best_state = ckpt['best_state']\n",
    "        rngstate = ckpt['rngs']\n",
    "        # Convert the state to a TrainState\n",
    "        self.best_loss = ckpt['best_loss']\n",
    "        print(\n",
    "            f\"Loaded model from checkpoint at epoch {epoch}\", ckpt['best_loss'])\n",
    "        return epoch, state, best_state, rngstate\n",
    "\n",
    "    def save(self, epoch=0):\n",
    "        print(f\"Saving model at epoch {epoch}\")\n",
    "        ckpt = {\n",
    "            # 'model': self.model,\n",
    "            'rngs': self.get_rngstate(),\n",
    "            'state': self.get_state(),\n",
    "            'best_state': self.get_best_state(),\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        try:\n",
    "            save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "            self.checkpointer.save(epoch, ckpt, save_kwargs={\n",
    "                                   'save_args': save_args}, force=True)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(\"Error saving checkpoint\", e)\n",
    "\n",
    "    def _define_train_step(self, **kwargs):\n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "        distributed_training = self.distributed_training\n",
    "\n",
    "        def train_step(train_state: SimpleTrainState, batch, rng_state: RandomMarkovState, local_device_indexes):\n",
    "            \"\"\"Train for a single step.\"\"\"\n",
    "            images = batch['image']\n",
    "            labels = batch['label']\n",
    "\n",
    "            def model_loss(params):\n",
    "                preds = model.apply(params, images)\n",
    "                expected_output = labels\n",
    "                nloss = loss_fn(preds, expected_output)\n",
    "                loss = jnp.mean(nloss)\n",
    "                return loss\n",
    "            loss, grads = jax.value_and_grad(model_loss)(train_state.params)\n",
    "            if distributed_training:\n",
    "                grads = jax.lax.pmean(grads, \"data\")\n",
    "            train_state = train_state.apply_gradients(grads=grads)\n",
    "            return train_state, loss, rng_state\n",
    "        \n",
    "        if distributed_training:\n",
    "            train_step = jax.pmap(train_step, axis_name=\"data\")\n",
    "            # train_step = shard_map(train_step, mesh=self.mesh, in_specs=P('data'), out_specs=P())\n",
    "        else:\n",
    "            train_step = jax.jit(train_step)\n",
    "            \n",
    "        return train_step\n",
    "\n",
    "    def _define_compute_metrics(self):\n",
    "        model = self.model\n",
    "        loss_fn = self.loss_fn\n",
    "\n",
    "        @jax.jit\n",
    "        def compute_metrics(state: SimpleTrainState, batch):\n",
    "            preds = model.apply(state.params, batch['image'])\n",
    "            expected_output = batch['label']\n",
    "            loss = jnp.mean(loss_fn(preds, expected_output))\n",
    "            metric_updates = state.metrics.single_from_model_output(\n",
    "                loss=loss, logits=preds, labels=expected_output)\n",
    "            metrics = state.metrics.merge(metric_updates)\n",
    "            state = state.replace(metrics=metrics)\n",
    "            return state\n",
    "        return compute_metrics\n",
    "\n",
    "    def summary(self):\n",
    "        input_vars = self.get_input_ones()\n",
    "        print(self.model.tabulate(jax.random.key(0), **input_vars,\n",
    "              console_kwargs={\"width\": 200, \"force_jupyter\": True, }))\n",
    "\n",
    "    def config(self):\n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"state\": self.state,\n",
    "            \"name\": self.name,\n",
    "            \"input_shapes\": self.input_shapes\n",
    "        }\n",
    "\n",
    "    def init_tensorboard(self, batch_size, steps_per_epoch, epochs):\n",
    "        summary_writer = tensorboard.SummaryWriter(self.tensorboard_path())\n",
    "        summary_writer.hparams({\n",
    "            **self.config(),\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size\n",
    "        })\n",
    "        return summary_writer\n",
    "\n",
    "    def fit(self, data, steps_per_epoch, epochs, train_step_args={}):\n",
    "        train_ds = iter(data['train']())\n",
    "        if 'test' in data:\n",
    "            test_ds = data['test']\n",
    "        else:\n",
    "            test_ds = None\n",
    "        train_step = self._define_train_step(**train_step_args)\n",
    "        compute_metrics = self._define_compute_metrics()\n",
    "        train_state = self.state\n",
    "        rng_state = self.rngstate\n",
    "        device_count = jax.local_device_count()\n",
    "        # train_ds = flax.jax_utils.prefetch_to_device(train_ds, jax.devices())\n",
    "        if self.distributed_training:\n",
    "            local_device_indexes = jnp.arange(device_count)\n",
    "        else:\n",
    "            local_device_indexes = 0\n",
    "\n",
    "        while self.latest_epoch < epochs:\n",
    "            self.latest_epoch += 1\n",
    "            current_epoch = self.latest_epoch\n",
    "            print(f\"\\nEpoch {current_epoch}/{epochs}\")\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "\n",
    "            with tqdm.tqdm(total=steps_per_epoch, desc=f'\\t\\tEpoch {current_epoch}', ncols=100, unit='step') as pbar:\n",
    "                for i in range(steps_per_epoch):\n",
    "                    batch = next(train_ds)\n",
    "                    if self.distributed_training and device_count > 1:\n",
    "                        batch = jax.tree.map(lambda x: x.reshape(\n",
    "                            (device_count, -1, *x.shape[1:])), batch)\n",
    "                        \n",
    "                    train_state, loss, rng_state = train_step(train_state, batch, rng_state, local_device_indexes)\n",
    "                    \n",
    "                    if self.distributed_training:\n",
    "                        loss = jax.experimental.multihost_utils.process_allgather(loss)\n",
    "                        loss = jnp.mean(loss)\n",
    "                    \n",
    "                    epoch_loss += loss\n",
    "                    if i % 100 == 0:\n",
    "                        pbar.set_postfix(loss=f'{loss:.4f}')\n",
    "                        pbar.update(100)\n",
    "                        current_step = current_epoch*steps_per_epoch + i\n",
    "                        if self.wandb is not None:\n",
    "                            self.wandb.log({\n",
    "                                \"train/step\" : current_step,\n",
    "                                \"train/loss\": loss,\n",
    "                            }, step=current_step)\n",
    "\n",
    "            print(f\"\\n\\tEpoch done\")\n",
    "            end_time = time.time()\n",
    "            self.state = train_state\n",
    "            self.rngstate = rng_state\n",
    "            total_time = end_time - start_time\n",
    "            avg_time_per_step = total_time / steps_per_epoch\n",
    "            avg_loss = epoch_loss / steps_per_epoch\n",
    "            if avg_loss < self.best_loss:\n",
    "                self.best_loss = avg_loss\n",
    "                self.best_state = train_state\n",
    "                self.save(current_epoch)\n",
    "            if self.wandb is not None:\n",
    "                self.wandb.log({\n",
    "                    \"train/epoch_time\": total_time,\n",
    "                    \"train/avg_time_per_step\": avg_time_per_step,\n",
    "                    \"train/avg_loss\": avg_loss,\n",
    "                    \"train/best_loss\": self.best_loss,\n",
    "                    \"train/epoch\": current_epoch,\n",
    "                }, step=current_step)\n",
    "\n",
    "            # Compute Metrics\n",
    "            metrics_str = ''\n",
    "\n",
    "            print(\n",
    "                f\"\\n\\tEpoch {current_epoch} completed. Avg Loss: {avg_loss}, Time: {total_time:.2f}s, Best Loss: {self.best_loss} {metrics_str}\")\n",
    "\n",
    "        self.save(epochs)\n",
    "        return self.state\n",
    "\n",
    "# Define the TrainState with EMA parameters\n",
    "\n",
    "class TrainState(SimpleTrainState):\n",
    "    rngs: jax.random.PRNGKey\n",
    "    ema_params: dict\n",
    "\n",
    "    def apply_ema(self, decay: float = 0.999):\n",
    "        new_ema_params = jax.tree_util.tree_map(\n",
    "            lambda ema, param: decay * ema + (1 - decay) * param,\n",
    "            self.ema_params,\n",
    "            self.params,\n",
    "        )\n",
    "        return self.replace(ema_params=new_ema_params)\n",
    "\n",
    "\n",
    "class DiffusionTrainer(SimpleTrainer):\n",
    "    noise_schedule: NoiseScheduler\n",
    "    model_output_transform: DiffusionPredictionTransform\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 input_shapes: Dict[str, Tuple[int]],\n",
    "                 optimizer: optax.GradientTransformation,\n",
    "                 noise_schedule: NoiseScheduler,\n",
    "                 rngs: jax.random.PRNGKey,\n",
    "                 unconditional_prob: float = 0.2,\n",
    "                 name: str = \"Diffusion\",\n",
    "                 model_output_transform: DiffusionPredictionTransform = EpsilonPredictionTransform(),\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            input_shapes=input_shapes,\n",
    "            optimizer=optimizer,\n",
    "            rngs=rngs,\n",
    "            name=name,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.model_output_transform = model_output_transform\n",
    "        self.unconditional_prob = unconditional_prob\n",
    "\n",
    "    def generate_states(\n",
    "        self,\n",
    "        optimizer: optax.GradientTransformation,\n",
    "        rngs: jax.random.PRNGKey,\n",
    "        existing_state: dict = None,\n",
    "        existing_best_state: dict = None,\n",
    "        model: nn.Module = None,\n",
    "        param_transforms: Callable = None\n",
    "    ) -> Tuple[TrainState, TrainState]:\n",
    "        print(\"Generating states for DiffusionTrainer\")\n",
    "        rngs, subkey = jax.random.split(rngs)\n",
    "\n",
    "        if existing_state == None:\n",
    "            input_vars = self.get_input_ones()\n",
    "            params = model.init(subkey, **input_vars)\n",
    "            new_state = {\"params\": params, \"ema_params\": params}\n",
    "        else:\n",
    "            new_state = existing_state\n",
    "\n",
    "        if param_transforms is not None:\n",
    "            params = param_transforms(params)\n",
    "\n",
    "        state = TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=new_state['params'],\n",
    "            ema_params=new_state['ema_params'],\n",
    "            tx=optimizer,\n",
    "            rngs=rngs,\n",
    "            metrics=Metrics.empty()\n",
    "        )\n",
    "            \n",
    "        if existing_best_state is not None:\n",
    "            best_state = state.replace(\n",
    "                params=existing_best_state['params'], ema_params=existing_best_state['ema_params'])\n",
    "        else:\n",
    "            best_state = state\n",
    "\n",
    "        return state, best_state\n",
    "\n",
    "    def _define_train_step(self, batch_size, null_labels_seq, text_embedder):\n",
    "        noise_schedule = self.noise_schedule\n",
    "        model = self.model\n",
    "        model_output_transform = self.model_output_transform\n",
    "        loss_fn = self.loss_fn\n",
    "        unconditional_prob = self.unconditional_prob\n",
    "\n",
    "        # Determine the number of unconditional samples\n",
    "        num_unconditional = int(batch_size * unconditional_prob)\n",
    "\n",
    "        nS, nC = null_labels_seq.shape\n",
    "        null_labels_seq = jnp.broadcast_to(\n",
    "            null_labels_seq, (batch_size, nS, nC))\n",
    "\n",
    "        distributed_training = self.distributed_training\n",
    "\n",
    "        # @jax.jit\n",
    "        def train_step(train_state: TrainState, batch, rng_state: RandomMarkovState, local_device_index):\n",
    "            \"\"\"Train for a single step.\"\"\"\n",
    "            images = batch['image']\n",
    "            # normalize image\n",
    "            images = (images - 127.5) / 127.5\n",
    "\n",
    "            output = text_embedder(\n",
    "                input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            label_seq = output.last_hidden_state\n",
    "\n",
    "            # Generate random probabilities to decide how much of this batch will be unconditional\n",
    "\n",
    "            label_seq = jnp.concat(\n",
    "                [null_labels_seq[:num_unconditional], label_seq[num_unconditional:]], axis=0)\n",
    "\n",
    "            rng_state, subkey = rng_state.get_random_key()\n",
    "            subkey = jax.random.fold_in(subkey, local_device_index)\n",
    "            subkey = jax.random.fold_in(subkey, jax.process_index())\n",
    "            local_rng_state = RandomMarkovState(subkey)\n",
    "\n",
    "            noise_level, local_rng_state = noise_schedule.generate_timesteps(images.shape[0], local_rng_state)\n",
    "            \n",
    "            local_rng_state, rngs = local_rng_state.get_random_key()\n",
    "            noise: jax.Array = jax.random.normal(rngs, shape=images.shape)\n",
    "            \n",
    "            rates = noise_schedule.get_rates(noise_level)\n",
    "            noisy_images, c_in, expected_output = model_output_transform.forward_diffusion(\n",
    "                images, noise, rates)\n",
    "\n",
    "            def model_loss(params):\n",
    "                preds = model.apply(\n",
    "                    params, *noise_schedule.transform_inputs(noisy_images*c_in, noise_level), label_seq)\n",
    "                preds = model_output_transform.pred_transform(\n",
    "                    noisy_images, preds, rates)\n",
    "                nloss = loss_fn(preds, expected_output)\n",
    "                # nloss = jnp.mean(nloss, axis=1)\n",
    "                nloss *= noise_schedule.get_weights(noise_level)\n",
    "                nloss = jnp.mean(nloss)\n",
    "                loss = nloss\n",
    "                return loss\n",
    "            \n",
    "            loss, grads = jax.value_and_grad(model_loss)(train_state.params)\n",
    "            if distributed_training:\n",
    "                grads = jax.lax.pmean(grads, \"data\")\n",
    "            train_state = train_state.apply_gradients(grads=grads)\n",
    "            train_state = train_state.apply_ema(self.ema_decay)\n",
    "            return train_state, loss, rng_state\n",
    "        \n",
    "        if distributed_training:\n",
    "            train_step = jax.pmap(train_step, axis_name=\"data\")\n",
    "            # train_step = shard_map(train_step, mesh=self.mesh, in_specs=P('data'), out_specs=P())\n",
    "        else:\n",
    "            train_step = jax.jit(train_step)\n",
    "            \n",
    "        return train_step\n",
    "\n",
    "    def _define_compute_metrics(self):\n",
    "        @jax.jit\n",
    "        def compute_metrics(state: TrainState, expected, pred):\n",
    "            loss = jnp.mean(jnp.square(pred - expected))\n",
    "            metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "            metrics = state.metrics.merge(metric_updates)\n",
    "            state = state.replace(metrics=metrics)\n",
    "            return state\n",
    "        return compute_metrics\n",
    "\n",
    "    def fit(self, data, steps_per_epoch, epochs):\n",
    "        null_labels_full = data['null_labels_full']\n",
    "        local_batch_size = data['local_batch_size']\n",
    "        text_embedder = data['model']\n",
    "        super().fit(data, steps_per_epoch, epochs, {\n",
    "            \"batch_size\": local_batch_size, \"null_labels_seq\": null_labels_full, \"text_embedder\": text_embedder})\n",
    "\n",
    "def boolean_string(s):\n",
    "    if type(s) == bool:\n",
    "        return s\n",
    "    return s == 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    #   \"dtype\": \"bfloat16\",\n",
    "    #   \"precision\": \"high\",\n",
    "    #   \"activation\": \"swish\",\n",
    "      \"emb_features\": 256,\n",
    "      \"feature_depths\": [\n",
    "        64,\n",
    "        128,\n",
    "        256,\n",
    "        512\n",
    "      ],\n",
    "      \"num_res_blocks\": 3,\n",
    "      \"attention_configs\": [\n",
    "        None,\n",
    "        {\n",
    "          \"dtype\": jnp.bfloat16,\n",
    "          \"heads\": 8,\n",
    "          \"use_projection\": False,\n",
    "          \"flash_attention\": False,\n",
    "          \"use_self_and_cross\": False\n",
    "        },\n",
    "        {\n",
    "          \"dtype\": jnp.bfloat16,\n",
    "          \"heads\": 8,\n",
    "          \"use_projection\": False,\n",
    "          \"flash_attention\": False,\n",
    "          \"use_self_and_cross\": False\n",
    "        },\n",
    "        {\n",
    "          \"dtype\": jnp.bfloat16,\n",
    "          \"heads\": 8,\n",
    "          \"use_projection\": False,\n",
    "          \"flash_attention\": False,\n",
    "          \"use_self_and_cross\": False\n",
    "        }\n",
    "      ],\n",
    "      \"num_middle_res_blocks\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrwhite0racle/.local/lib/python3.10/site-packages/orbax/checkpoint/type_handlers.py:1442: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint at epoch 5 0.045471836\n",
      "Generating states for DiffusionTrainer\n",
      "Replicating state across devices  [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = CONFIG\n",
    "IMAGE_SIZE=128\n",
    "# model_config['activation'] = ACTIVATION_MAP[model_config['activation']]\n",
    "# model_config['dtype'] = DTYPE_MAP[model_config['dtype']]\n",
    "# model_config['precision'] = PRECISION_MAP[model_config['precision']]\n",
    "unet = Unet(activation=jax.nn.swish, dtype=jnp.bfloat16, precision=jax.lax.Precision.HIGH, **model_config)\n",
    "solver = optax.adam(2e-4)\n",
    "\n",
    "edm_schedule = EDMNoiseScheduler(1, sigma_max=80, rho=7, sigma_data=0.5)\n",
    "karas_ve_schedule = KarrasVENoiseScheduler(1, sigma_max=80, rho=7, sigma_data=0.5)\n",
    "\n",
    "trainer = DiffusionTrainer(unet, optimizer=solver, \n",
    "                           noise_schedule=edm_schedule,\n",
    "                           rngs=jax.random.PRNGKey(4), \n",
    "                           name=\"CC12M Raw: batch size of 64\",\n",
    "                           model_output_transform=KarrasPredictionTransform(sigma_data=edm_schedule.sigma_data),\n",
    "                           input_shapes= {\n",
    "                                \"x\": (128, 128, 3),\n",
    "                                \"temb\": (),\n",
    "                                \"textcontext\": (77, 768)\n",
    "                            },\n",
    "                        #    train_state=trainer.best_state,\n",
    "                        #    loss_fn=lambda x, y: jnp.abs(x - y),\n",
    "                            # param_transforms=params_transform,\n",
    "                           load_from_checkpoint=True,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaxdiff.utils import clip_images\n",
    "\n",
    "def clip_images(images, clip_min=-1, clip_max=1):\n",
    "    return jnp.clip(images, clip_min, clip_max)\n",
    "    \n",
    "class DiffusionSampler():\n",
    "    model:nn.Module\n",
    "    noise_schedule:NoiseScheduler\n",
    "    params:dict\n",
    "    model_output_transform:DiffusionPredictionTransform\n",
    "\n",
    "    def __init__(self, model:nn.Module, params:dict,  \n",
    "                 noise_schedule:NoiseScheduler, \n",
    "                 model_output_transform:DiffusionPredictionTransform=EpsilonPredictionTransform(),\n",
    "                 guidance_scale:float = 0.0,\n",
    "                 null_labels_seq:jax.Array=None\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.params = params\n",
    "        self.model_output_transform = model_output_transform\n",
    "        self.guidance_scale = guidance_scale\n",
    "        if self.guidance_scale > 0:\n",
    "            # Classifier free guidance\n",
    "            assert null_labels_seq is not None, \"Null labels sequence is required for classifier-free guidance\"\n",
    "            print(\"Using classifier-free guidance\")\n",
    "            @jax.jit\n",
    "            def sample_model(x_t, t, *additional_inputs):\n",
    "                # Concatenate unconditional and conditional inputs\n",
    "                x_t_cat = jnp.concatenate([x_t] * 2, axis=0)\n",
    "                t_cat = jnp.concatenate([t] * 2, axis=0)\n",
    "                rates_cat = self.noise_schedule.get_rates(t_cat)\n",
    "                c_in_cat = self.model_output_transform.get_input_scale(rates_cat)\n",
    "                \n",
    "                text_labels_seq, = additional_inputs\n",
    "                text_labels_seq = jnp.concatenate([text_labels_seq, jnp.broadcast_to(null_labels_seq, text_labels_seq.shape)], axis=0)\n",
    "                model_output = self.model.apply(self.params, *self.noise_schedule.transform_inputs(x_t_cat * c_in_cat, t_cat), text_labels_seq)\n",
    "                # Split model output into unconditional and conditional parts\n",
    "                model_output_cond, model_output_uncond = jnp.split(model_output, 2, axis=0)\n",
    "                model_output = model_output_uncond + guidance_scale * (model_output_cond - model_output_uncond)\n",
    "                \n",
    "                x_0, eps = self.model_output_transform(x_t, model_output, t, self.noise_schedule)\n",
    "                return x_0, eps, model_output\n",
    "            \n",
    "            self.sample_model = sample_model\n",
    "        else:\n",
    "            # Unconditional sampling\n",
    "            @jax.jit\n",
    "            def sample_model(x_t, t, *additional_inputs):\n",
    "                rates = self.noise_schedule.get_rates(t)\n",
    "                c_in = self.model_output_transform.get_input_scale(rates)\n",
    "                model_output = self.model.apply(self.params, *self.noise_schedule.transform_inputs(x_t * c_in, t), *additional_inputs)\n",
    "                x_0, eps = self.model_output_transform(x_t, model_output, t, self.noise_schedule)\n",
    "                return x_0, eps, model_output\n",
    "            \n",
    "            self.sample_model = sample_model\n",
    "\n",
    "    # Used to sample from the diffusion model\n",
    "    def sample_step(self, current_samples:jnp.ndarray, current_step, model_conditioning_inputs, next_step=None, state:MarkovState=None) -> tuple[jnp.ndarray, MarkovState]:\n",
    "        # First clip the noisy images\n",
    "        step_ones = jnp.ones((current_samples.shape[0], ), dtype=jnp.int32)\n",
    "        current_step = step_ones * current_step\n",
    "        next_step = step_ones * next_step\n",
    "        pred_images, pred_noise, _ = self.sample_model(current_samples, current_step, *model_conditioning_inputs)\n",
    "        # plotImages(pred_images)\n",
    "        pred_images = clip_images(pred_images)\n",
    "        new_samples, state =  self.take_next_step(current_samples=current_samples, reconstructed_samples=pred_images, \n",
    "                             pred_noise=pred_noise, current_step=current_step, next_step=next_step, state=state,\n",
    "                             model_conditioning_inputs=model_conditioning_inputs\n",
    "                             )\n",
    "        return new_samples, state\n",
    "\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        # estimate the q(x_{t-1} | x_t, x_0). \n",
    "        # pred_images is x_0, noisy_images is x_t, steps is t\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def scale_steps(self, steps):\n",
    "        scale_factor = self.noise_schedule.max_timesteps / 1000\n",
    "        return steps * scale_factor\n",
    "\n",
    "    def get_steps(self, start_step, end_step, diffusion_steps):\n",
    "        step_range = start_step - end_step\n",
    "        if diffusion_steps is None or diffusion_steps == 0:\n",
    "            diffusion_steps = start_step - end_step\n",
    "        diffusion_steps = min(diffusion_steps, step_range)\n",
    "        steps = jnp.linspace(end_step, start_step, diffusion_steps, dtype=jnp.int16)[::-1]\n",
    "        return steps\n",
    "    \n",
    "    def get_initial_samples(self, num_images, rngs:jax.random.PRNGKey, start_step):\n",
    "        start_step = self.scale_steps(start_step)\n",
    "        alpha_n, sigma_n = self.noise_schedule.get_rates(start_step)\n",
    "        variance = jnp.sqrt(alpha_n ** 2 + sigma_n ** 2) \n",
    "        return jax.random.normal(rngs, (num_images, IMAGE_SIZE, IMAGE_SIZE, 3)) * variance\n",
    "\n",
    "    def generate_images(self,\n",
    "                        num_images=16, \n",
    "                        diffusion_steps=1000, \n",
    "                        start_step:int = None,\n",
    "                        end_step:int = 0,\n",
    "                        steps_override=None,\n",
    "                        priors=None, \n",
    "                        rngstate:RandomMarkovState=RandomMarkovState(jax.random.PRNGKey(42)),\n",
    "                        model_conditioning_inputs:tuple=()\n",
    "                        ) -> jnp.ndarray:\n",
    "        if priors is None:\n",
    "            rngstate, newrngs = rngstate.get_random_key()\n",
    "            samples = self.get_initial_samples(num_images, newrngs, start_step)\n",
    "        else:\n",
    "            print(\"Using priors\")\n",
    "            samples = priors\n",
    "\n",
    "        # @jax.jit\n",
    "        def sample_step(state:RandomMarkovState, samples, current_step, next_step):\n",
    "            samples, state = self.sample_step(current_samples=samples,\n",
    "                                              current_step=current_step, \n",
    "                                              model_conditioning_inputs=model_conditioning_inputs,\n",
    "                                              state=state, next_step=next_step)\n",
    "            return samples, state\n",
    "\n",
    "        if start_step is None:\n",
    "            start_step = self.noise_schedule.max_timesteps\n",
    "\n",
    "        if steps_override is not None:\n",
    "            steps = steps_override\n",
    "        else:\n",
    "            steps = self.get_steps(start_step, end_step, diffusion_steps)\n",
    "\n",
    "        # print(\"Sampling steps\", steps)\n",
    "        for i in tqdm.tqdm(range(0, len(steps))):\n",
    "            current_step = self.scale_steps(steps[i])\n",
    "            next_step = self.scale_steps(steps[i+1] if i+1 < len(steps) else 0)\n",
    "            if i != len(steps) - 1:\n",
    "                # print(\"normal step\")\n",
    "                samples, rngstate = sample_step(rngstate, samples, current_step, next_step)\n",
    "            else:\n",
    "                # print(\"last step\")\n",
    "                step_ones = jnp.ones((num_images, ), dtype=jnp.int32)\n",
    "                samples, _, _ = self.sample_model(samples, current_step * step_ones, *model_conditioning_inputs)\n",
    "        samples = clip_images(samples)\n",
    "        return samples\n",
    "\n",
    "class DDPMSampler(DiffusionSampler):\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        mean = self.noise_schedule.get_posterior_mean(reconstructed_samples, current_samples, current_step)\n",
    "        variance = self.noise_schedule.get_posterior_variance(steps=current_step)\n",
    "        \n",
    "        state, rng = state.get_random_key()\n",
    "        # Now sample from the posterior\n",
    "        noise = jax.random.normal(rng, reconstructed_samples.shape, dtype=jnp.float32)\n",
    "\n",
    "        return mean + noise * variance, state\n",
    "    \n",
    "    def generate_images(self, num_images=16, diffusion_steps=1000, start_step: int = None, *args, **kwargs):\n",
    "        return super().generate_images(num_images=num_images, diffusion_steps=diffusion_steps, start_step=start_step, *args, **kwargs)\n",
    "    \n",
    "class SimpleDDPMSampler(DiffusionSampler):\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        state, rng = state.get_random_key()\n",
    "        noise = jax.random.normal(rng, reconstructed_samples.shape, dtype=jnp.float32)\n",
    "\n",
    "        # Compute noise rates and signal rates only once\n",
    "        current_signal_rate, current_noise_rate = self.noise_schedule.get_rates(current_step)\n",
    "        next_signal_rate, next_noise_rate = self.noise_schedule.get_rates(next_step)\n",
    "        \n",
    "        pred_noise_coeff = ((next_noise_rate ** 2) * current_signal_rate) / (current_noise_rate * next_signal_rate)\n",
    "        \n",
    "        noise_ratio_squared = (next_noise_rate ** 2) / (current_noise_rate ** 2)\n",
    "        signal_ratio_squared = (current_signal_rate ** 2) / (next_signal_rate ** 2)\n",
    "        gamma = jnp.sqrt(noise_ratio_squared * (1 - signal_ratio_squared))\n",
    "        \n",
    "        next_samples = next_signal_rate * reconstructed_samples + pred_noise_coeff * pred_noise + noise * gamma\n",
    "        return next_samples, state\n",
    "\n",
    "class DDIMSampler(DiffusionSampler):\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        next_signal_rate, next_noise_rate = self.noise_schedule.get_rates(next_step)\n",
    "        return reconstructed_samples * next_signal_rate + pred_noise * next_noise_rate, state\n",
    "    \n",
    "class EulerSampler(DiffusionSampler):\n",
    "    # Basically a DDIM Sampler but parameterized as an ODE\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        current_alpha, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        next_alpha, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        dt = next_sigma - current_sigma\n",
    "        \n",
    "        x_0_coeff = (current_alpha * next_sigma - next_alpha * current_sigma) / (dt)\n",
    "        dx = (current_samples - x_0_coeff * reconstructed_samples) / current_sigma\n",
    "        next_samples = current_samples + dx * dt\n",
    "        return next_samples, state\n",
    "\n",
    "class SimplifiedEulerSampler(DiffusionSampler):\n",
    "    \"\"\"\n",
    "    This is for networks with forward diffusion of the form x_{t+1} = x_t + sigma_t * epsilon_t\n",
    "    \"\"\"\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        _, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        _, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        dt = next_sigma - current_sigma\n",
    "        \n",
    "        dx = (current_samples - reconstructed_samples) / current_sigma\n",
    "        next_samples = current_samples + dx * dt\n",
    "        return next_samples, state\n",
    "    \n",
    "class HeunSampler(DiffusionSampler):\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        # Get the noise and signal rates for the current and next steps\n",
    "        current_alpha, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        next_alpha, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        dt = next_sigma - current_sigma\n",
    "        x_0_coeff = (current_alpha * next_sigma - next_alpha * current_sigma) / dt\n",
    "\n",
    "        dx_0 = (current_samples - x_0_coeff * reconstructed_samples) / current_sigma\n",
    "        next_samples_0 = current_samples + dx_0 * dt\n",
    "        \n",
    "        # Recompute x_0 and eps at the first estimate to refine the derivative\n",
    "        estimated_x_0, _, _ = self.sample_model(next_samples_0, next_step, *model_conditioning_inputs)\n",
    "        \n",
    "        # Estimate the refined derivative using the midpoint (Heun's method)\n",
    "        dx_1 = (next_samples_0 - x_0_coeff * estimated_x_0) / next_sigma\n",
    "        # Compute the final next samples by averaging the initial and refined derivatives\n",
    "        final_next_samples = current_samples + 0.5 * (dx_0 + dx_1) * dt\n",
    "        \n",
    "        return final_next_samples, state\n",
    "\n",
    "class RK4Sampler(DiffusionSampler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert issubclass(type(self.noise_schedule), GeneralizedNoiseScheduler), \"Noise schedule must be a GeneralizedNoiseScheduler\"\n",
    "        @jax.jit\n",
    "        def get_derivative(x_t, sigma, state:RandomMarkovState, model_conditioning_inputs) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "            t = self.noise_schedule.get_timesteps(sigma)\n",
    "            x_0, eps, _ = self.sample_model(x_t, t, *model_conditioning_inputs)\n",
    "            return eps, state\n",
    "        \n",
    "        self.get_derivative = get_derivative\n",
    "\n",
    "    def sample_step(self, current_samples:jnp.ndarray, current_step, model_conditioning_inputs, next_step=None, state:MarkovState=None) -> tuple[jnp.ndarray, MarkovState]:\n",
    "        step_ones = jnp.ones((current_samples.shape[0], ), dtype=jnp.int32)\n",
    "        current_step = step_ones * current_step\n",
    "        next_step = step_ones * next_step\n",
    "        _, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        _, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        dt = next_sigma - current_sigma\n",
    "\n",
    "        k1, state = self.get_derivative(current_samples, current_sigma, state, model_conditioning_inputs)\n",
    "        k2, state = self.get_derivative(current_samples + 0.5 * k1 * dt, current_sigma + 0.5 * dt, state, model_conditioning_inputs)\n",
    "        k3, state = self.get_derivative(current_samples + 0.5 * k2 * dt, current_sigma + 0.5 * dt, state, model_conditioning_inputs)\n",
    "        k4, state = self.get_derivative(current_samples + k3 * dt, current_sigma + dt, state, model_conditioning_inputs)\n",
    "\n",
    "        next_samples = current_samples + (((k1 + 2 * k2 + 2 * k3 + k4) * dt) / 6)\n",
    "        return next_samples, state\n",
    "\n",
    "class MultiStepDPM(DiffusionSampler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.history = []\n",
    "\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        # Get the noise and signal rates for the current and next steps\n",
    "        current_alpha, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        next_alpha, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        dt = next_sigma - current_sigma\n",
    "\n",
    "        def first_order(current_noise, current_sigma):\n",
    "            dx = current_noise\n",
    "            return dx\n",
    "        \n",
    "        def second_order(current_noise, current_sigma, last_noise, last_sigma):\n",
    "            dx_2 = (current_noise - last_noise) / (current_sigma - last_sigma)\n",
    "            return dx_2\n",
    "        \n",
    "        def third_order(current_noise, current_sigma, last_noise, last_sigma, second_last_noise, second_last_sigma):\n",
    "            dx_2 = second_order(current_noise, current_sigma, last_noise, last_sigma)\n",
    "            dx_2_last = second_order(last_noise, last_sigma, second_last_noise, second_last_sigma)\n",
    "\n",
    "            dx_3 = (dx_2 - dx_2_last) / (0.5 * ((current_sigma + last_sigma) - (last_sigma + second_last_sigma)))\n",
    "            \n",
    "            return dx_3\n",
    "\n",
    "        if len(self.history) == 0:\n",
    "            # First order only\n",
    "            dx_1 = first_order(pred_noise, current_sigma)\n",
    "            next_samples = current_samples + dx_1 * dt\n",
    "        elif len(self.history) == 1:\n",
    "            # First + Second order\n",
    "            dx_1 = first_order(pred_noise, current_sigma)\n",
    "            last_step = self.history[-1]\n",
    "            dx_2 = second_order(pred_noise, current_sigma, last_step['eps'], last_step['sigma'])\n",
    "            next_samples = current_samples + dx_1 * dt + 0.5 * dx_2 * dt**2\n",
    "        else:\n",
    "            # First + Second + Third order\n",
    "            last_step = self.history[-1]\n",
    "            second_last_step = self.history[-2]\n",
    "\n",
    "            dx_1 = first_order(pred_noise, current_sigma)\n",
    "            dx_2 = second_order(pred_noise, current_sigma, last_step['eps'], last_step['sigma'])\n",
    "            dx_3 = third_order(pred_noise, current_sigma, last_step['eps'], last_step['sigma'], second_last_step['eps'], second_last_step['sigma'])\n",
    "            next_samples = current_samples + (dx_1 * dt) + (0.5 * dx_2 * dt**2) + ((1/6) * dx_3 * dt**3)\n",
    "\n",
    "        self.history.append({\n",
    "            \"eps\": pred_noise,\n",
    "            \"sigma\" : current_sigma,\n",
    "        })\n",
    "        return next_samples, state\n",
    "    \n",
    "class EulerAncestralSampler(DiffusionSampler):\n",
    "    def take_next_step(self, current_samples, reconstructed_samples, model_conditioning_inputs,\n",
    "                 pred_noise, current_step, state:RandomMarkovState, next_step=1) -> tuple[jnp.ndarray, RandomMarkovState]:\n",
    "        current_alpha, current_sigma = self.noise_schedule.get_rates(current_step)\n",
    "        next_alpha, next_sigma = self.noise_schedule.get_rates(next_step)\n",
    "\n",
    "        sigma_up = (next_sigma**2 * (current_sigma**2 - next_sigma**2) / current_sigma**2) ** 0.5\n",
    "        sigma_down = (next_sigma**2 - sigma_up**2) ** 0.5\n",
    "        \n",
    "        dt = sigma_down - current_sigma\n",
    "        \n",
    "        x_0_coeff = (current_alpha * next_sigma - next_alpha * current_sigma) / (next_sigma - current_sigma)\n",
    "        dx = (current_samples - x_0_coeff * reconstructed_samples) / current_sigma\n",
    "        \n",
    "        state, subkey = state.get_random_key()\n",
    "        dW = jax.random.normal(subkey, current_samples.shape) * sigma_up\n",
    "        \n",
    "        next_samples = current_samples + dx * dt + dW\n",
    "        return next_samples, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing FlaxCLIPTextModel: {('vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'kernel'), ('text_projection', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'kernel'), ('logit_scale',), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'post_layernorm', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'post_layernorm', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'pre_layrnorm', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'embeddings', 'position_embedding', 'embedding'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'kernel'), ('visual_projection', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'embeddings', 'class_embedding'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'bias'), ('vision_model', 'embeddings', 'patch_embedding', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'kernel'), ('vision_model', 'pre_layrnorm', 'scale'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def defaultTextEncodeModel():\n",
    "    modelname = \"openai/clip-vit-large-patch14\"\n",
    "    model = FlaxCLIPTextModel.from_pretrained(modelname, dtype=jnp.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname, dtype=jnp.float16)\n",
    "    return model, tokenizer\n",
    "\n",
    "def encodePrompts(prompts, model, tokenizer=None):\n",
    "    if model == None:\n",
    "        model, tokenizer = defaultTextEncodeModel()\n",
    "    if tokenizer == None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"np\")\n",
    "    inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooler_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "    embed_pooled = pooler_output.astype(jnp.float16)\n",
    "    embed_labels_full = last_hidden_state.astype(jnp.float16)\n",
    "    \n",
    "    return embed_pooled, embed_labels_full\n",
    "\n",
    "textEncoderModel, textTokenizer = defaultTextEncodeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_labels, null_labels_full = encodePrompts([\"\"], textEncoderModel, textTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using classifier-free guidance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                 | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "prompts = [\n",
    "    'water tulip',\n",
    "    'a water lily',\n",
    "    'a water lily', \n",
    "    'a photo of a rose',\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"a big house\",\n",
    "    \"a big house\",\n",
    "    \"a big house\",\n",
    "    \"a beautiful landscape\",\n",
    "    \"a beautiful landscape\",\n",
    "    \"a beautiful landscape\",\n",
    "    \"a beautiful landscape\",\n",
    "    \"a beautiful landscape\",\n",
    "    ]\n",
    "pooled_labels, labels_seq = encodePrompts(prompts, textEncoderModel, textTokenizer)\n",
    "\n",
    "sampler = EulerAncestralSampler(trainer.model, trainer.get_state().ema_params, karas_ve_schedule, model_output_transform=trainer.model_output_transform, guidance_scale=2, null_labels_seq=null_labels_full)\n",
    "samples = sampler.generate_images(num_images=len(prompts), diffusion_steps=200, start_step=1000, end_step=0, priors=None, model_conditioning_inputs=(labels_seq,))\n",
    "plotImages(samples, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
