{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f95b1a3",
   "metadata": {},
   "source": [
    "# Video Diffusion with FlaxUNet3DConditionModel\n",
    "\n",
    "This notebook demonstrates how to use the FlaxUNet3DConditionModel for video diffusion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788e6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrwhite0racle/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flaxdiff.schedulers import EDMNoiseScheduler, KarrasVENoiseScheduler\n",
    "from flaxdiff.predictors import KarrasPredictionTransform\n",
    "from flaxdiff.models.simple_unet import Unet\n",
    "from flaxdiff.trainer.general_diffusion_trainer import GeneralDiffusionTrainer, ConditionalInputConfig\n",
    "from flaxdiff.data.datasets import get_dataset_grain, get_media_dataset_grain\n",
    "from flaxdiff.utils import defaultTextEncodeModel\n",
    "from flaxdiff.models.autoencoder.diffusers import StableDiffusionVAE\n",
    "from flaxdiff.samplers.euler import EulerAncestralSampler\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = get_media_dataset_grain(\"ucf101\", batch_size=BATCH_SIZE, media_scale=IMAGE_SIZE)\n",
    "datalen = data['train_len']\n",
    "batches = datalen // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02327607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data['train']())\n",
    "batch = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83da25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4381abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:55:42.973940: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744556142.998162  192486 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744556143.005274  192486 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744556143.022849  192486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744556143.022872  192486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744556143.022874  192486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744556143.022876  192486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing FlaxCLIPTextModel: {('vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'bias'), ('vision_model', 'pre_layrnorm', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'scale'), ('vision_model', 'embeddings', 'patch_embedding', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'bias'), ('logit_scale',), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc1', 'kernel'), ('vision_model', 'post_layernorm', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'embeddings', 'class_embedding'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '18', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '14', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'kernel'), ('vision_model', 'pre_layrnorm', 'scale'), ('visual_projection', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '14', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '9', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '13', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('text_projection', 'kernel'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '11', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '12', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '23', 'layer_norm2', 'bias'), ('vision_model', 'embeddings', 'position_embedding', 'embedding'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '23', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '7', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '22', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '17', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '17', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '2', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '13', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '22', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '20', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '18', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc1', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc1', 'kernel'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '1', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '0', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '5', 'layer_norm2', 'bias'), ('vision_model', 'encoder', 'layers', '16', 'self_attn', 'q_proj', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'post_layernorm', 'scale'), ('vision_model', 'encoder', 'layers', '19', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '3', 'self_attn', 'k_proj', 'bias'), ('vision_model', 'encoder', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '10', 'layer_norm1', 'scale'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm1', 'bias'), ('vision_model', 'encoder', 'layers', '15', 'self_attn', 'out_proj', 'bias'), ('vision_model', 'encoder', 'layers', '19', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '9', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '2', 'self_attn', 'out_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '4', 'mlp', 'fc2', 'bias'), ('vision_model', 'encoder', 'layers', '8', 'layer_norm2', 'scale'), ('vision_model', 'encoder', 'layers', '21', 'self_attn', 'v_proj', 'bias'), ('vision_model', 'encoder', 'layers', '10', 'mlp', 'fc2', 'kernel'), ('vision_model', 'encoder', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'encoder', 'layers', '3', 'mlp', 'fc2', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/mrwhite0racle/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factor: 0.18215\n",
      "Calculating downscale factor...\n",
      "Downscale factor: 8\n",
      "Latent channels: 4\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = get_dataset_grain(\"oxford_flowers102\", batch_size=BATCH_SIZE, image_scale=IMAGE_SIZE)\n",
    "datalen = data['train_len']\n",
    "batches = datalen // BATCH_SIZE\n",
    "\n",
    "text_encoder = defaultTextEncodeModel()\n",
    "autoencoder = StableDiffusionVAE(**{\"modelname\": \"pcuenq/sd-vae-ft-mse-flax\"})\n",
    "\n",
    "# Construct a validation set by the prompts\n",
    "val_prompts = ['water tulip', ' a water lily', ' a water lily', ' a photo of a rose', ' a photo of a rose', ' a water lily', ' a water lily', ' a photo of a marigold', ' a photo of a marigold', ' a photo of a marigold', ' a water lily', ' a photo of a sunflower', ' a photo of a lotus', ' columbine', ' columbine', ' an orchid', ' an orchid', ' an orchid', ' a water lily', ' a water lily', ' a water lily', ' columbine', ' columbine', ' a photo of a sunflower', ' a photo of a sunflower', ' a photo of a sunflower', ' a photo of a lotus', ' a photo of a lotus', ' a photo of a marigold', ' a photo of a marigold', ' a photo of a rose', ' a photo of a rose', ' a photo of a rose', ' orange dahlia', ' orange dahlia', ' a lenten rose', ' a lenten rose', ' a water lily', ' a water lily', ' a water lily', ' a water lily', ' an orchid', ' an orchid', ' an orchid', ' hard-leaved pocket orchid', ' bird of paradise', ' bird of paradise', ' a photo of a lovely rose', ' a photo of a lovely rose', ' a photo of a globe-flower', ' a photo of a globe-flower', ' a photo of a lovely rose', ' a photo of a lovely rose', ' a photo of a ruby-lipped cattleya', ' a photo of a ruby-lipped cattleya', ' a photo of a lovely rose', ' a water lily', ' a osteospermum', ' a osteospermum', ' a water lily', ' a water lily', ' a water lily', ' a red rose', ' a red rose']\n",
    "\n",
    "def get_val_dataset(batch_size=8):\n",
    "    for i in range(0, len(val_prompts), batch_size):\n",
    "        prompts = val_prompts[i:i + batch_size]\n",
    "        tokens = text_encoder.tokenize(prompts)\n",
    "        yield {\"text\": tokens}\n",
    "\n",
    "data['test'] = get_val_dataset\n",
    "data['test_len'] = len(val_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data['train']())\n",
    "batch = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706c7ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 256, 256, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de21008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac8daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffd1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c374389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 20,027,236 parameters\n"
     ]
    }
   ],
   "source": [
    "def create_model(rng):\n",
    "    num_frames = 8\n",
    "    model = FlaxUNet3DConditionModel(\n",
    "        sample_size=(num_frames, 32, 32),\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        down_block_types=(\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"DownBlock3D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "        ),\n",
    "        block_out_channels=(32, 64, 128, 256),\n",
    "        layers_per_block=1,\n",
    "        cross_attention_dim=64,\n",
    "        attention_head_dim=8,\n",
    "        dropout=0.0,\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Create dummy inputs for initialization\n",
    "    batch_size = 1\n",
    "    sample = jax.random.normal(\n",
    "        rng, \n",
    "        shape=(batch_size, num_frames, 32, 32, 4),\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    timestep = jnp.array([0], dtype=jnp.int32)\n",
    "    \n",
    "    # Create dummy text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng, \n",
    "        shape=(batch_size, 77, 64),  # 77 is standard for CLIP text tokens\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Initialize the model\n",
    "    params = model.init(rng, sample, timestep, encoder_hidden_states)\n",
    "    \n",
    "    # Print model summary\n",
    "    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"Model initialized with {param_count:,} parameters\")\n",
    "    \n",
    "    return model, params\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, model_rng = jax.random.split(rng)\n",
    "model, params = create_model(model_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4065755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8, 32, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "sample_video = np.random.rand(2, 8, 32, 32, 4).astype(np.float32)\n",
    "sample_video = jnp.array(sample_video)\n",
    "timestep = jnp.ones((2,), dtype=jnp.int32) * 0\n",
    "    \n",
    "# Create dummy text embeddings\n",
    "encoder_hidden_states = jax.random.normal(\n",
    "    rng, \n",
    "    shape=(2, 77, 64),  # 77 is standard for CLIP text tokens\n",
    "    dtype=jnp.bfloat16\n",
    ")\n",
    "\n",
    "out = model.apply(\n",
    "    params,\n",
    "    sample_video,\n",
    "    timestep,\n",
    "    encoder_hidden_states,\n",
    "    return_dict=True\n",
    ")\n",
    "print(out.shape)  # Should be (2, 8, 32, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc70490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9960cf",
   "metadata": {},
   "source": [
    "## 2. Set up the Diffusion Process\n",
    "\n",
    "Now we'll set up the noise scheduler and sampler for our diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a052fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noise scheduler\n",
    "noise_scheduler = EDMNoiseScheduler(1, sigma_min=0.002, sigma_max=80.0, rho=7.0)\n",
    "\n",
    "# Create a prediction transform\n",
    "model_output_transform = EpsilonPredictionTransform()\n",
    "\n",
    "# Create a sampler\n",
    "sampler = EulerSampler(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    noise_schedule=noise_scheduler,\n",
    "    model_output_transform=model_output_transform,\n",
    "    guidance_scale=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53a402",
   "metadata": {},
   "source": [
    "## 3. Generate a Simple Video\n",
    "\n",
    "Let's generate a simple random video using our model. For a real application, you would use a text encoder like CLIP to encode prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1420622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 8 frames with 20 diffusion steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (3, 3, 3, 4, 32) but got shape (3, 3, 3, 3, 32) instead for parameter \"kernel\" in \"/conv_in\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScopeParamShapeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m video\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Generate video\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m video = \u001b[43mgenerate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_video\u001b[39m\u001b[34m(num_frames, height, width, steps)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate video frames\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m frames with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m diffusion steps...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m video = \u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:335\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples\u001b[39m\u001b[34m(self, params, batch_size, sequence_length, diffusion_steps, start_step, end_step, steps_override, priors, rngstate, model_conditioning_inputs)\u001b[39m\n\u001b[32m    332\u001b[39m next_step = \u001b[38;5;28mself\u001b[39m.scale_steps(steps[i+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(steps) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(steps) - \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     samples, rngstate = \u001b[43msample_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_step\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     step_ones = jnp.ones((samples.shape[\u001b[32m0\u001b[39m],), dtype=jnp.int32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:312\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples.<locals>.sample_step\u001b[39m\u001b[34m(sample_model_fn, state, samples, current_step, next_step)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_step\u001b[39m(sample_model_fn, state: RandomMarkovState, samples, current_step, next_step):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     samples, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_step\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m samples, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:141\u001b[39m, in \u001b[36mDiffusionSampler.sample_step\u001b[39m\u001b[34m(self, sample_model_fn, current_samples, current_step, model_conditioning_inputs, next_step, state)\u001b[39m\n\u001b[32m    138\u001b[39m current_step = step_ones * current_step\n\u001b[32m    139\u001b[39m next_step = step_ones * next_step\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m pred_images, pred_noise, _ = \u001b[43msample_model_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m new_samples, state = \u001b[38;5;28mself\u001b[39m.take_next_step(\n\u001b[32m    146\u001b[39m     current_samples=current_samples,\n\u001b[32m    147\u001b[39m     reconstructed_samples=pred_images,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     sample_model_fn=sample_model_fn,\n\u001b[32m    154\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_samples, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:309\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples.<locals>.sample_model_fn\u001b[39m\u001b[34m(x_t, t, *additional_inputs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_model_fn\u001b[39m(x_t, t, *additional_inputs):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:95\u001b[39m, in \u001b[36mDiffusionSampler.__init__.<locals>.sample_model\u001b[39m\u001b[34m(params, x_t, t, *additional_inputs)\u001b[39m\n\u001b[32m     93\u001b[39m rates = \u001b[38;5;28mself\u001b[39m.noise_schedule.get_rates(t)\n\u001b[32m     94\u001b[39m c_in = \u001b[38;5;28mself\u001b[39m.model_output_transform.get_input_scale(rates)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_inputs\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m x_0, eps = \u001b[38;5;28mself\u001b[39m.model_output_transform(x_t, model_output, t, \u001b[38;5;28mself\u001b[39m.noise_schedule)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_0, eps, model_output\n",
      "    \u001b[31m[... skipping hidden 6 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/models/unet_3d.py:346\u001b[39m, in \u001b[36mFlaxUNet3DConditionModel.__call__\u001b[39m\u001b[34m(self, sample, timesteps, encoder_hidden_states, frame_encoder_hidden_states, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict, train)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# 2. Pre-process input - reshape from [B, F, H, W, C] to [B*F, H, W, C] for 2D operations\u001b[39;00m\n\u001b[32m    345\u001b[39m sample = sample.reshape(batch * num_frames, height, width, channels)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Process encoder hidden states - repeat for each frame and combine with frame-specific conditioning if provided\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    350\u001b[39m     \u001b[38;5;66;03m# Repeat video-wide conditioning for each frame: (B, S, X) -> (B*F, S, X)\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/flax/linen/linear.py:662\u001b[39m, in \u001b[36m_Conv.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask.shape != kernel_shape:\n\u001b[32m    657\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    658\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMask needs to have the same shape as weights. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.mask.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    660\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkernel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_dtype\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    667\u001b[39m   kernel *= \u001b[38;5;28mself\u001b[39m.mask\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/flax/core/scope.py:960\u001b[39m, in \u001b[36mScope.param\u001b[39m\u001b[34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[39m\n\u001b[32m    955\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[32m    956\u001b[39m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[32m    957\u001b[39m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[32m    958\u001b[39m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.shape(val) != np.shape(abs_val):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m errors.ScopeParamShapeError(\n\u001b[32m    961\u001b[39m         name, \u001b[38;5;28mself\u001b[39m.path_text, np.shape(abs_val), np.shape(val)\n\u001b[32m    962\u001b[39m       )\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    964\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_mutable_collection(\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[31mScopeParamShapeError\u001b[39m: Initializer expected to generate shape (3, 3, 3, 4, 32) but got shape (3, 3, 3, 3, 32) instead for parameter \"kernel\" in \"/conv_in\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "def generate_video(num_frames=8, height=32, width=32, steps=20):\n",
    "    # Create mock text embeddings (in a real scenario, you'd use a text encoder like CLIP)\n",
    "    batch_size = 1\n",
    "    rng_gen = jax.random.PRNGKey(123)  # Using a different seed\n",
    "    \n",
    "    # Generate random text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng_gen, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate video frames\n",
    "    print(f\"Generating {num_frames} frames with {steps} diffusion steps...\")\n",
    "    video = sampler.generate_images(\n",
    "        params=params,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=num_frames,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        model_conditioning_inputs=(encoder_hidden_states,),\n",
    "    )\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Generate video\n",
    "video = generate_video(num_frames=8, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf7f9d",
   "metadata": {},
   "source": [
    "## 4. Visualize the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd541d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_video(video):\n",
    "    # Normalize to [0, 1] range for visualization\n",
    "    video_clip = np.array(video[0])\n",
    "    video_clip = (video_clip + 1.0) / 2.0  # Assuming [-1, 1] range\n",
    "    video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "    \n",
    "    # Only use RGB channels (first 3) for visualization\n",
    "    video_clip = video_clip[:, :, :, :3]\n",
    "    \n",
    "    # Create a figure for animation\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create initial frame\n",
    "    img = ax.imshow(video_clip[0])\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        img.set_array(video_clip[i])\n",
    "        return [img]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(video_clip), interval=200, blit=True\n",
    "    )\n",
    "    \n",
    "    # Display the animation\n",
    "    from IPython.display import HTML\n",
    "    HTML(anim.to_jshtml())\n",
    "    \n",
    "    # Also display individual frames for reference\n",
    "    fig, axes = plt.subplots(1, len(video_clip), figsize=(15, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(video_clip[i])\n",
    "        ax.set_title(f\"Frame {i}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# Visualize the generated video\n",
    "anim = visualize_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083082f",
   "metadata": {},
   "source": [
    "## 5. Save the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeebe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video, filename='generated_video.mp4'):\n",
    "    video_clip = np.array(video[0])\n",
    "    video_clip = (video_clip + 1.0) / 2.0  # Assuming [-1, 1] range\n",
    "    video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "    \n",
    "    # Only use RGB channels (first 3) for saving\n",
    "    video_clip = video_clip[:, :, :, :3]\n",
    "    \n",
    "    # Create a figure for animation\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create initial frame\n",
    "    img = ax.imshow(video_clip[0])\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        img.set_array(video_clip[i])\n",
    "        return [img]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(video_clip), interval=200, blit=True\n",
    "    )\n",
    "    \n",
    "    # Save the animation\n",
    "    anim.save(filename, writer='ffmpeg', fps=5, dpi=100)\n",
    "    print(f\"Video saved to {filename}\")\n",
    "    \n",
    "    # Also save individual frames\n",
    "    for i, frame in enumerate(video_clip):\n",
    "        plt.imsave(f\"frame_{i}.png\", frame)\n",
    "    \n",
    "# Comment out if you don't have ffmpeg installed\n",
    "# save_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e05bb7",
   "metadata": {},
   "source": [
    "## 6. Experiment with Different Parameters\n",
    "\n",
    "Let's experiment with different guidance scales to see how they affect the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac61cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_guidance_scale(guidance_scales=[1.0, 3.0, 5.0, 7.0], num_frames=8, steps=20):\n",
    "    results = {}\n",
    "    \n",
    "    for gs in guidance_scales:\n",
    "        print(f\"Generating video with guidance scale {gs}...\")\n",
    "        \n",
    "        # Create a sampler with the current guidance scale\n",
    "        temp_sampler = EulerSampler(\n",
    "            model=model,\n",
    "            params=params,\n",
    "            noise_schedule=noise_scheduler,\n",
    "            model_output_transform=model_output_transform,\n",
    "            guidance_scale=gs,\n",
    "        )\n",
    "        \n",
    "        # Create mock text embeddings\n",
    "        batch_size = 1\n",
    "        rng_gen = jax.random.PRNGKey(123)  # Using a consistent seed for comparison\n",
    "        \n",
    "        encoder_hidden_states = jax.random.normal(\n",
    "            rng_gen, \n",
    "            shape=(batch_size, 77, 64),\n",
    "            dtype=jnp.float32\n",
    "        )\n",
    "        \n",
    "        # Generate video\n",
    "        video = temp_sampler.generate_images(\n",
    "            params=params,\n",
    "            num_images=batch_size,\n",
    "            diffusion_steps=steps,\n",
    "            start_step=1000,\n",
    "            end_step=0,\n",
    "            priors=None,\n",
    "            image_shape=(num_frames, 32, 32, 4),\n",
    "            model_conditioning_inputs=(encoder_hidden_states,),\n",
    "        )\n",
    "        \n",
    "        results[gs] = video\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run the experiment\n",
    "# guidance_results = experiment_with_guidance_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fdb01",
   "metadata": {},
   "source": [
    "## 7. Processing Existing Video\n",
    "\n",
    "In a real-world scenario, you might want to process existing video frames. Here's how you could do that with the UNet3D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2672c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_existing_video(video_frames, noise_level=0.2):\n",
    "    \"\"\"\n",
    "    Process existing video frames with the UNet3D model.\n",
    "    This is a simple example that adds noise and then denoises.\n",
    "    \n",
    "    Args:\n",
    "        video_frames: numpy array of shape (num_frames, height, width, channels)\n",
    "        noise_level: Amount of noise to add (0-1)\n",
    "    \"\"\"\n",
    "    # Convert to JAX array and ensure correct shape\n",
    "    video_frames = jnp.array(video_frames)\n",
    "    batch_size = 1\n",
    "    num_frames, height, width, channels = video_frames.shape\n",
    "    \n",
    "    # Scale to [-1, 1] if needed\n",
    "    if video_frames.max() > 1.0:\n",
    "        video_frames = video_frames / 255.0\n",
    "    if video_frames.max() <= 1.0 and video_frames.min() >= 0.0:\n",
    "        video_frames = video_frames * 2.0 - 1.0\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    video_frames = video_frames.reshape(batch_size, num_frames, height, width, channels)\n",
    "    \n",
    "    # If channels < 4, pad with zeros\n",
    "    if channels < 4:\n",
    "        padding = jnp.zeros((batch_size, num_frames, height, width, 4 - channels))\n",
    "        video_frames = jnp.concatenate([video_frames, padding], axis=-1)\n",
    "    \n",
    "    # Add noise\n",
    "    rng_noise = jax.random.PRNGKey(456)\n",
    "    noise = jax.random.normal(rng_noise, video_frames.shape)\n",
    "    noisy_frames = video_frames + noise_level * noise\n",
    "    \n",
    "    # Create mock text embeddings \n",
    "    rng_text = jax.random.PRNGKey(789)\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng_text, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    print(\"Processing video...\")\n",
    "    \n",
    "    # For a simple demonstration, we'll just do a single denoising step\n",
    "    timestep = jnp.array([500], dtype=jnp.int32)  # Middle of the diffusion process\n",
    "    output = model.apply(params, noisy_frames, timestep, encoder_hidden_states)\n",
    "    \n",
    "    # Extract the first 3 channels for visualization\n",
    "    processed_frames = output['sample'][0, :, :, :, :3]\n",
    "    original_frames = video_frames[0, :, :, :, :3]\n",
    "    noisy_frames = noisy_frames[0, :, :, :, :3]\n",
    "    \n",
    "    # Normalize to [0, 1] for visualization\n",
    "    processed_frames = (processed_frames + 1.0) / 2.0\n",
    "    original_frames = (original_frames + 1.0) / 2.0\n",
    "    noisy_frames = (noisy_frames + 1.0) / 2.0\n",
    "    \n",
    "    processed_frames = jnp.clip(processed_frames, 0.0, 1.0)\n",
    "    original_frames = jnp.clip(original_frames, 0.0, 1.0)\n",
    "    noisy_frames = jnp.clip(noisy_frames, 0.0, 1.0)\n",
    "    \n",
    "    return {\n",
    "        'original': original_frames,\n",
    "        'noisy': noisy_frames,\n",
    "        'processed': processed_frames\n",
    "    }\n",
    "\n",
    "# Create some synthetic video frames for demonstration\n",
    "def create_synthetic_video(num_frames=8, height=32, width=32):\n",
    "    \"\"\"Create a simple synthetic video with moving shapes\"\"\"\n",
    "    frames = np.zeros((num_frames, height, width, 3))\n",
    "    \n",
    "    # Add a moving circle\n",
    "    for i in range(num_frames):\n",
    "        # Create frame with a circle\n",
    "        frame = np.zeros((height, width, 3))\n",
    "        x_center = width // 2 + int(width * 0.3 * np.sin(i / num_frames * 2 * np.pi))\n",
    "        y_center = height // 2 + int(height * 0.3 * np.cos(i / num_frames * 2 * np.pi))\n",
    "        \n",
    "        # Draw circle\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                dist = np.sqrt((x - x_center)**2 + (y - y_center)**2)\n",
    "                if dist < 5:\n",
    "                    frame[y, x, 0] = 1.0  # Red circle\n",
    "        \n",
    "        # Add a static square\n",
    "        frame[5:15, 5:15, 1] = 1.0  # Green square\n",
    "        \n",
    "        frames[i] = frame\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Generate synthetic video and process it\n",
    "synthetic_video = create_synthetic_video()\n",
    "# Uncomment to process the video\n",
    "# processed_results = process_existing_video(synthetic_video, noise_level=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49f699",
   "metadata": {},
   "source": [
    "## 8. Using Frame-Specific Conditioning\n",
    "\n",
    "The UNet3D model now supports both video-wide conditioning and optional frame-specific conditioning. Let's see how to use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_frame_conditioning(num_frames=8, height=32, width=32, steps=20):\n",
    "    # Create batch\n",
    "    batch_size = 1\n",
    "    rng_gen = jax.random.PRNGKey(123)\n",
    "    rng_gen, key1, key2 = jax.random.split(rng_gen, 3)\n",
    "    \n",
    "    # Generate random global text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        key1, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate random frame-specific embeddings\n",
    "    frame_encoder_hidden_states = jax.random.normal(\n",
    "        key2, \n",
    "        shape=(batch_size, num_frames, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate video frames - demonstrate with and without frame conditioning\n",
    "    print(f\"Generating {num_frames} frames with global conditioning only...\")\n",
    "    video_global = sampler.generate_images(\n",
    "        params=params,\n",
    "        num_images=batch_size,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        image_shape=(num_frames, height, width, 4),\n",
    "        model_conditioning_inputs=(encoder_hidden_states,),\n",
    "    )\n",
    "    \n",
    "    print(f\"Generating {num_frames} frames with global + frame-specific conditioning...\")\n",
    "    video_combined = sampler.generate_images(\n",
    "        params=params,\n",
    "        num_images=batch_size,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        image_shape=(num_frames, height, width, 4),\n",
    "        model_conditioning_inputs=(encoder_hidden_states, frame_encoder_hidden_states),\n",
    "    )\n",
    "    \n",
    "    return video_global, video_combined\n",
    "\n",
    "# Uncomment to run the experiment\n",
    "# video_global, video_combined = generate_with_frame_conditioning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_videos(video1, video2, title1=\"Global Conditioning\", title2=\"Global + Frame Conditioning\"):\n",
    "    # Normalize both videos\n",
    "    def normalize_video(video):\n",
    "        video_clip = np.array(video[0])\n",
    "        video_clip = (video_clip + 1.0) / 2.0\n",
    "        video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "        video_clip = video_clip[:, :, :, :3]  # RGB only\n",
    "        return video_clip\n",
    "    \n",
    "    video1_norm = normalize_video(video1)\n",
    "    video2_norm = normalize_video(video2)\n",
    "    \n",
    "    # Display side by side frames\n",
    "    num_frames = video1_norm.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_frames, figsize=(num_frames*2, 4))\n",
    "    \n",
    "    # Display first video on top row\n",
    "    for i in range(num_frames):\n",
    "        axes[0, i].imshow(video1_norm[i])\n",
    "        axes[0, i].set_title(f\"Frame {i}\")\n",
    "        axes[0, i].axis('off')\n",
    "    axes[0, 0].set_ylabel(title1)\n",
    "    \n",
    "    # Display second video on bottom row\n",
    "    for i in range(num_frames):\n",
    "        axes[1, i].imshow(video2_norm[i])\n",
    "        axes[1, i].set_title(f\"Frame {i}\")\n",
    "        axes[1, i].axis('off')\n",
    "    axes[1, 0].set_ylabel(title2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to compare the videos\n",
    "# if 'video_global' in locals() and 'video_combined' in locals():\n",
    "#     compare_videos(video_global, video_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7057ecb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "1. How to initialize and use the FlaxUNet3DConditionModel\n",
    "2. How to generate new videos from random noise\n",
    "3. How to modify existing videos using the model\n",
    "4. How to use frame-specific conditioning for more detailed control\n",
    "\n",
    "The FlaxUNet3DConditionModel provides a powerful tool for video diffusion tasks, offering the performance benefits of JAX and Flax while maintaining compatibility with diffusers-style APIs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaxdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
