{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f95b1a3",
   "metadata": {},
   "source": [
    "# Video Diffusion with FlaxUNet3DConditionModel\n",
    "\n",
    "This notebook demonstrates how to use the FlaxUNet3DConditionModel for video diffusion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788e6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrwhite0racle/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mrwhite0racle/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n",
      "2025-04-13 03:38:51.106856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744515531.131056  144427 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744515531.138472  144427 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744515531.156062  144427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744515531.156084  144427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744515531.156087  144427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744515531.156089  144427 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import FlaxDiff components\n",
    "from flaxdiff.models.unet_3d import FlaxUNet3DConditionModel\n",
    "from flaxdiff.schedulers import EDMNoiseScheduler\n",
    "from flaxdiff.predictors import EpsilonPredictionTransform\n",
    "from flaxdiff.samplers.ddpm import DDPMSampler\n",
    "from flaxdiff.samplers.euler import EulerSampler\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51a06b",
   "metadata": {},
   "source": [
    "## 1. Define the Model\n",
    "\n",
    "First, we'll define and initialize our UNet3D model. For demonstration purposes, we'll use a smaller model than would typically be used for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 20,027,236 parameters\n"
     ]
    }
   ],
   "source": [
    "def create_model(rng):\n",
    "    num_frames = 8\n",
    "    model = FlaxUNet3DConditionModel(\n",
    "        sample_size=(num_frames, 32, 32),\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        down_block_types=(\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"DownBlock3D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "        ),\n",
    "        block_out_channels=(32, 64, 128, 256),\n",
    "        layers_per_block=1,\n",
    "        cross_attention_dim=64,\n",
    "        attention_head_dim=8,\n",
    "        dropout=0.0,\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Create dummy inputs for initialization\n",
    "    batch_size = 1\n",
    "    sample = jax.random.normal(\n",
    "        rng, \n",
    "        shape=(batch_size, num_frames, 32, 32, 4),\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    timestep = jnp.array([0], dtype=jnp.int32)\n",
    "    \n",
    "    # Create dummy text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng, \n",
    "        shape=(batch_size, 77, 64),  # 77 is standard for CLIP text tokens\n",
    "        dtype=jnp.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Initialize the model\n",
    "    params = model.init(rng, sample, timestep, encoder_hidden_states)\n",
    "    \n",
    "    # Print model summary\n",
    "    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"Model initialized with {param_count:,} parameters\")\n",
    "    \n",
    "    return model, params\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, model_rng = jax.random.split(rng)\n",
    "model, params = create_model(model_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4065755",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (16, 32, 32, 32), (2, 1, 1, 32).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create dummy text embeddings\u001b[39;00m\n\u001b[32m      6\u001b[39m encoder_hidden_states = jax.random.normal(\n\u001b[32m      7\u001b[39m     rng, \n\u001b[32m      8\u001b[39m     shape=(\u001b[32m2\u001b[39m, \u001b[32m77\u001b[39m, \u001b[32m64\u001b[39m),  \u001b[38;5;66;03m# 77 is standard for CLIP text tokens\u001b[39;00m\n\u001b[32m      9\u001b[39m     dtype=jnp.bfloat16\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(out[\u001b[33m\"\u001b[39m\u001b[33msample\u001b[39m\u001b[33m\"\u001b[39m].shape)  \u001b[38;5;66;03m# Should be (2, 8, 32, 32, 4)\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 6 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/models/unet_3d.py:385\u001b[39m, in \u001b[36mFlaxUNet3DConditionModel.__call__\u001b[39m\u001b[34m(self, sample, timesteps, encoder_hidden_states, frame_encoder_hidden_states, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict, train)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.down_blocks:\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(down_block, FlaxCrossAttnDownBlock3D):\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sample, res_samples = \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    393\u001b[39m         sample, res_samples = down_block(\n\u001b[32m    394\u001b[39m             sample, \n\u001b[32m    395\u001b[39m             t_emb, \n\u001b[32m    396\u001b[39m             num_frames=num_frames, \n\u001b[32m    397\u001b[39m             deterministic=\u001b[38;5;129;01mnot\u001b[39;00m train\n\u001b[32m    398\u001b[39m         )\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/models/unet_3d_blocks.py:235\u001b[39m, in \u001b[36mFlaxCrossAttnDownBlock3D.__call__\u001b[39m\u001b[34m(self, hidden_states, temb, encoder_hidden_states, num_frames, deterministic)\u001b[39m\n\u001b[32m    232\u001b[39m output_states = ()\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m resnet, temp_conv, attn, temp_attn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.resnets, \u001b[38;5;28mself\u001b[39m.temp_convs, \u001b[38;5;28mself\u001b[39m.attentions, \u001b[38;5;28mself\u001b[39m.temp_attentions):\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     hidden_states = temp_conv(hidden_states, num_frames=num_frames, deterministic=deterministic)\n\u001b[32m    237\u001b[39m     hidden_states = attn(hidden_states, encoder_hidden_states, deterministic=deterministic)\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/diffusers/models/resnet_flax.py:114\u001b[39m, in \u001b[36mFlaxResnetBlock2D.__call__\u001b[39m\u001b[34m(self, hidden_states, temb, deterministic)\u001b[39m\n\u001b[32m    112\u001b[39m temb = \u001b[38;5;28mself\u001b[39m.time_emb_proj(nn.swish(temb))\n\u001b[32m    113\u001b[39m temb = jnp.expand_dims(jnp.expand_dims(temb, \u001b[32m1\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m hidden_states = \u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\n\u001b[32m    116\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm2(hidden_states)\n\u001b[32m    117\u001b[39m hidden_states = nn.swish(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:579\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    577\u001b[39m args = (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/jax/_src/numpy/ufunc_api.py:180\u001b[39m, in \u001b[36mufunc.__call__\u001b[39m\u001b[34m(self, out, where, *args)\u001b[39m\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m call = \u001b[38;5;28mself\u001b[39m.__static_props[\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_vectorized\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:1215\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x, y)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add two arrays element-wise.\u001b[39;00m\n\u001b[32m   1190\u001b[39m \n\u001b[32m   1191\u001b[39m \u001b[33;03mJAX implementation of :obj:`numpy.add`. This is a universal function,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1212\u001b[39m \u001b[33;03m  Array([10, 11, 12, 13], dtype=int32)\u001b[39;00m\n\u001b[32m   1213\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1214\u001b[39m x, y = promote_args(\u001b[33m\"\u001b[39m\u001b[33madd\u001b[39m\u001b[33m\"\u001b[39m, x, y)\n\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x.dtype != \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax.bitwise_or(x, y)\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/jax/_src/lax/lax.py:135\u001b[39m, in \u001b[36m_try_broadcast_shapes\u001b[39m\u001b[34m(name, *shapes)\u001b[39m\n\u001b[32m    133\u001b[39m       result_shape.append(non_1s[\u001b[32m0\u001b[39m])\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m got incompatible shapes for broadcasting: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    136\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[31mTypeError\u001b[39m: add got incompatible shapes for broadcasting: (16, 32, 32, 32), (2, 1, 1, 32)."
     ]
    }
   ],
   "source": [
    "sample_video = np.random.rand(2, 8, 32, 32, 4).astype(np.float32)\n",
    "sample_video = jnp.array(sample_video)\n",
    "timestep = jnp.ones((2,), dtype=jnp.int32) * 0\n",
    "    \n",
    "# Create dummy text embeddings\n",
    "encoder_hidden_states = jax.random.normal(\n",
    "    rng, \n",
    "    shape=(2, 77, 64),  # 77 is standard for CLIP text tokens\n",
    "    dtype=jnp.bfloat16\n",
    ")\n",
    "\n",
    "out = model.apply(\n",
    "    params,\n",
    "    sample_video,\n",
    "    timestep,\n",
    "    encoder_hidden_states,\n",
    "    return_dict=True\n",
    ")\n",
    "print(out[\"sample\"].shape)  # Should be (2, 8, 32, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc70490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c9960cf",
   "metadata": {},
   "source": [
    "## 2. Set up the Diffusion Process\n",
    "\n",
    "Now we'll set up the noise scheduler and sampler for our diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a052fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a noise scheduler\n",
    "noise_scheduler = EDMNoiseScheduler(1, sigma_min=0.002, sigma_max=80.0, rho=7.0)\n",
    "\n",
    "# Create a prediction transform\n",
    "model_output_transform = EpsilonPredictionTransform()\n",
    "\n",
    "# Create a sampler\n",
    "sampler = EulerSampler(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    noise_schedule=noise_scheduler,\n",
    "    model_output_transform=model_output_transform,\n",
    "    guidance_scale=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53a402",
   "metadata": {},
   "source": [
    "## 3. Generate a Simple Video\n",
    "\n",
    "Let's generate a simple random video using our model. For a real application, you would use a text encoder like CLIP to encode prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1420622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 8 frames with 20 diffusion steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (3, 3, 3, 4, 32) but got shape (3, 3, 3, 3, 32) instead for parameter \"kernel\" in \"/conv_in\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScopeParamShapeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m video\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Generate video\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m video = \u001b[43mgenerate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_video\u001b[39m\u001b[34m(num_frames, height, width, steps)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate video frames\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m frames with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m diffusion steps...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m video = \u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m video\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:335\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples\u001b[39m\u001b[34m(self, params, batch_size, sequence_length, diffusion_steps, start_step, end_step, steps_override, priors, rngstate, model_conditioning_inputs)\u001b[39m\n\u001b[32m    332\u001b[39m next_step = \u001b[38;5;28mself\u001b[39m.scale_steps(steps[i+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i+\u001b[32m1\u001b[39m < \u001b[38;5;28mlen\u001b[39m(steps) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(steps) - \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     samples, rngstate = \u001b[43msample_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_step\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     step_ones = jnp.ones((samples.shape[\u001b[32m0\u001b[39m],), dtype=jnp.int32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:312\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples.<locals>.sample_step\u001b[39m\u001b[34m(sample_model_fn, state, samples, current_step, next_step)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_step\u001b[39m(sample_model_fn, state: RandomMarkovState, samples, current_step, next_step):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     samples, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_model_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_step\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m samples, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:141\u001b[39m, in \u001b[36mDiffusionSampler.sample_step\u001b[39m\u001b[34m(self, sample_model_fn, current_samples, current_step, model_conditioning_inputs, next_step, state)\u001b[39m\n\u001b[32m    138\u001b[39m current_step = step_ones * current_step\n\u001b[32m    139\u001b[39m next_step = step_ones * next_step\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m pred_images, pred_noise, _ = \u001b[43msample_model_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_conditioning_inputs\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m new_samples, state = \u001b[38;5;28mself\u001b[39m.take_next_step(\n\u001b[32m    146\u001b[39m     current_samples=current_samples,\n\u001b[32m    147\u001b[39m     reconstructed_samples=pred_images,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     sample_model_fn=sample_model_fn,\n\u001b[32m    154\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_samples, state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:309\u001b[39m, in \u001b[36mDiffusionSampler.generate_samples.<locals>.sample_model_fn\u001b[39m\u001b[34m(x_t, t, *additional_inputs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_model_fn\u001b[39m(x_t, t, *additional_inputs):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/samplers/common.py:95\u001b[39m, in \u001b[36mDiffusionSampler.__init__.<locals>.sample_model\u001b[39m\u001b[34m(params, x_t, t, *additional_inputs)\u001b[39m\n\u001b[32m     93\u001b[39m rates = \u001b[38;5;28mself\u001b[39m.noise_schedule.get_rates(t)\n\u001b[32m     94\u001b[39m c_in = \u001b[38;5;28mself\u001b[39m.model_output_transform.get_input_scale(rates)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnoise_schedule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_inputs\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m x_0, eps = \u001b[38;5;28mself\u001b[39m.model_output_transform(x_t, model_output, t, \u001b[38;5;28mself\u001b[39m.noise_schedule)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_0, eps, model_output\n",
      "    \u001b[31m[... skipping hidden 6 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/persist/FlaxDiff/flaxdiff/models/unet_3d.py:355\u001b[39m, in \u001b[36mFlaxUNet3DConditionModel.__call__\u001b[39m\u001b[34m(self, sample, timesteps, encoder_hidden_states, frame_encoder_hidden_states, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict, train)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# 2. Pre-process input - reshape from [B, F, H, W, C] to [B*F, H, W, C] for 2D operations\u001b[39;00m\n\u001b[32m    354\u001b[39m sample = sample.reshape(batch * num_frames, height, width, channels)\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# Process encoder hidden states - repeat for each frame and combine with frame-specific conditioning if provided\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m     \u001b[38;5;66;03m# Repeat video-wide conditioning for each frame: (B, S, X) -> (B*F, S, X)\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/flax/linen/linear.py:662\u001b[39m, in \u001b[36m_Conv.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask.shape != kernel_shape:\n\u001b[32m    657\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    658\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMask needs to have the same shape as weights. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.mask.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    660\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkernel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_dtype\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    667\u001b[39m   kernel *= \u001b[38;5;28mself\u001b[39m.mask\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flaxdiff/lib/python3.11/site-packages/flax/core/scope.py:960\u001b[39m, in \u001b[36mScope.param\u001b[39m\u001b[34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[39m\n\u001b[32m    955\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[32m    956\u001b[39m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[32m    957\u001b[39m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[32m    958\u001b[39m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.shape(val) != np.shape(abs_val):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m errors.ScopeParamShapeError(\n\u001b[32m    961\u001b[39m         name, \u001b[38;5;28mself\u001b[39m.path_text, np.shape(abs_val), np.shape(val)\n\u001b[32m    962\u001b[39m       )\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    964\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_mutable_collection(\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[31mScopeParamShapeError\u001b[39m: Initializer expected to generate shape (3, 3, 3, 4, 32) but got shape (3, 3, 3, 3, 32) instead for parameter \"kernel\" in \"/conv_in\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "def generate_video(num_frames=8, height=32, width=32, steps=20):\n",
    "    # Create mock text embeddings (in a real scenario, you'd use a text encoder like CLIP)\n",
    "    batch_size = 1\n",
    "    rng_gen = jax.random.PRNGKey(123)  # Using a different seed\n",
    "    \n",
    "    # Generate random text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng_gen, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate video frames\n",
    "    print(f\"Generating {num_frames} frames with {steps} diffusion steps...\")\n",
    "    video = sampler.generate_images(\n",
    "        params=params,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=num_frames,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        model_conditioning_inputs=(encoder_hidden_states,),\n",
    "    )\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Generate video\n",
    "video = generate_video(num_frames=8, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf7f9d",
   "metadata": {},
   "source": [
    "## 4. Visualize the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd541d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_video(video):\n",
    "    # Normalize to [0, 1] range for visualization\n",
    "    video_clip = np.array(video[0])\n",
    "    video_clip = (video_clip + 1.0) / 2.0  # Assuming [-1, 1] range\n",
    "    video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "    \n",
    "    # Only use RGB channels (first 3) for visualization\n",
    "    video_clip = video_clip[:, :, :, :3]\n",
    "    \n",
    "    # Create a figure for animation\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create initial frame\n",
    "    img = ax.imshow(video_clip[0])\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        img.set_array(video_clip[i])\n",
    "        return [img]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(video_clip), interval=200, blit=True\n",
    "    )\n",
    "    \n",
    "    # Display the animation\n",
    "    from IPython.display import HTML\n",
    "    HTML(anim.to_jshtml())\n",
    "    \n",
    "    # Also display individual frames for reference\n",
    "    fig, axes = plt.subplots(1, len(video_clip), figsize=(15, 3))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(video_clip[i])\n",
    "        ax.set_title(f\"Frame {i}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return anim\n",
    "\n",
    "# Visualize the generated video\n",
    "anim = visualize_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083082f",
   "metadata": {},
   "source": [
    "## 5. Save the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeebe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video, filename='generated_video.mp4'):\n",
    "    video_clip = np.array(video[0])\n",
    "    video_clip = (video_clip + 1.0) / 2.0  # Assuming [-1, 1] range\n",
    "    video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "    \n",
    "    # Only use RGB channels (first 3) for saving\n",
    "    video_clip = video_clip[:, :, :, :3]\n",
    "    \n",
    "    # Create a figure for animation\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create initial frame\n",
    "    img = ax.imshow(video_clip[0])\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        img.set_array(video_clip[i])\n",
    "        return [img]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, animate, frames=len(video_clip), interval=200, blit=True\n",
    "    )\n",
    "    \n",
    "    # Save the animation\n",
    "    anim.save(filename, writer='ffmpeg', fps=5, dpi=100)\n",
    "    print(f\"Video saved to {filename}\")\n",
    "    \n",
    "    # Also save individual frames\n",
    "    for i, frame in enumerate(video_clip):\n",
    "        plt.imsave(f\"frame_{i}.png\", frame)\n",
    "    \n",
    "# Comment out if you don't have ffmpeg installed\n",
    "# save_video(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e05bb7",
   "metadata": {},
   "source": [
    "## 6. Experiment with Different Parameters\n",
    "\n",
    "Let's experiment with different guidance scales to see how they affect the generated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac61cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_guidance_scale(guidance_scales=[1.0, 3.0, 5.0, 7.0], num_frames=8, steps=20):\n",
    "    results = {}\n",
    "    \n",
    "    for gs in guidance_scales:\n",
    "        print(f\"Generating video with guidance scale {gs}...\")\n",
    "        \n",
    "        # Create a sampler with the current guidance scale\n",
    "        temp_sampler = EulerSampler(\n",
    "            model=model,\n",
    "            params=params,\n",
    "            noise_schedule=noise_scheduler,\n",
    "            model_output_transform=model_output_transform,\n",
    "            guidance_scale=gs,\n",
    "        )\n",
    "        \n",
    "        # Create mock text embeddings\n",
    "        batch_size = 1\n",
    "        rng_gen = jax.random.PRNGKey(123)  # Using a consistent seed for comparison\n",
    "        \n",
    "        encoder_hidden_states = jax.random.normal(\n",
    "            rng_gen, \n",
    "            shape=(batch_size, 77, 64),\n",
    "            dtype=jnp.float32\n",
    "        )\n",
    "        \n",
    "        # Generate video\n",
    "        video = temp_sampler.generate_images(\n",
    "            params=params,\n",
    "            num_images=batch_size,\n",
    "            diffusion_steps=steps,\n",
    "            start_step=1000,\n",
    "            end_step=0,\n",
    "            priors=None,\n",
    "            image_shape=(num_frames, 32, 32, 4),\n",
    "            model_conditioning_inputs=(encoder_hidden_states,),\n",
    "        )\n",
    "        \n",
    "        results[gs] = video\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run the experiment\n",
    "# guidance_results = experiment_with_guidance_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fdb01",
   "metadata": {},
   "source": [
    "## 7. Processing Existing Video\n",
    "\n",
    "In a real-world scenario, you might want to process existing video frames. Here's how you could do that with the UNet3D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2672c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_existing_video(video_frames, noise_level=0.2):\n",
    "    \"\"\"\n",
    "    Process existing video frames with the UNet3D model.\n",
    "    This is a simple example that adds noise and then denoises.\n",
    "    \n",
    "    Args:\n",
    "        video_frames: numpy array of shape (num_frames, height, width, channels)\n",
    "        noise_level: Amount of noise to add (0-1)\n",
    "    \"\"\"\n",
    "    # Convert to JAX array and ensure correct shape\n",
    "    video_frames = jnp.array(video_frames)\n",
    "    batch_size = 1\n",
    "    num_frames, height, width, channels = video_frames.shape\n",
    "    \n",
    "    # Scale to [-1, 1] if needed\n",
    "    if video_frames.max() > 1.0:\n",
    "        video_frames = video_frames / 255.0\n",
    "    if video_frames.max() <= 1.0 and video_frames.min() >= 0.0:\n",
    "        video_frames = video_frames * 2.0 - 1.0\n",
    "    \n",
    "    # Add a batch dimension\n",
    "    video_frames = video_frames.reshape(batch_size, num_frames, height, width, channels)\n",
    "    \n",
    "    # If channels < 4, pad with zeros\n",
    "    if channels < 4:\n",
    "        padding = jnp.zeros((batch_size, num_frames, height, width, 4 - channels))\n",
    "        video_frames = jnp.concatenate([video_frames, padding], axis=-1)\n",
    "    \n",
    "    # Add noise\n",
    "    rng_noise = jax.random.PRNGKey(456)\n",
    "    noise = jax.random.normal(rng_noise, video_frames.shape)\n",
    "    noisy_frames = video_frames + noise_level * noise\n",
    "    \n",
    "    # Create mock text embeddings \n",
    "    rng_text = jax.random.PRNGKey(789)\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        rng_text, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    print(\"Processing video...\")\n",
    "    \n",
    "    # For a simple demonstration, we'll just do a single denoising step\n",
    "    timestep = jnp.array([500], dtype=jnp.int32)  # Middle of the diffusion process\n",
    "    output = model.apply(params, noisy_frames, timestep, encoder_hidden_states)\n",
    "    \n",
    "    # Extract the first 3 channels for visualization\n",
    "    processed_frames = output['sample'][0, :, :, :, :3]\n",
    "    original_frames = video_frames[0, :, :, :, :3]\n",
    "    noisy_frames = noisy_frames[0, :, :, :, :3]\n",
    "    \n",
    "    # Normalize to [0, 1] for visualization\n",
    "    processed_frames = (processed_frames + 1.0) / 2.0\n",
    "    original_frames = (original_frames + 1.0) / 2.0\n",
    "    noisy_frames = (noisy_frames + 1.0) / 2.0\n",
    "    \n",
    "    processed_frames = jnp.clip(processed_frames, 0.0, 1.0)\n",
    "    original_frames = jnp.clip(original_frames, 0.0, 1.0)\n",
    "    noisy_frames = jnp.clip(noisy_frames, 0.0, 1.0)\n",
    "    \n",
    "    return {\n",
    "        'original': original_frames,\n",
    "        'noisy': noisy_frames,\n",
    "        'processed': processed_frames\n",
    "    }\n",
    "\n",
    "# Create some synthetic video frames for demonstration\n",
    "def create_synthetic_video(num_frames=8, height=32, width=32):\n",
    "    \"\"\"Create a simple synthetic video with moving shapes\"\"\"\n",
    "    frames = np.zeros((num_frames, height, width, 3))\n",
    "    \n",
    "    # Add a moving circle\n",
    "    for i in range(num_frames):\n",
    "        # Create frame with a circle\n",
    "        frame = np.zeros((height, width, 3))\n",
    "        x_center = width // 2 + int(width * 0.3 * np.sin(i / num_frames * 2 * np.pi))\n",
    "        y_center = height // 2 + int(height * 0.3 * np.cos(i / num_frames * 2 * np.pi))\n",
    "        \n",
    "        # Draw circle\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                dist = np.sqrt((x - x_center)**2 + (y - y_center)**2)\n",
    "                if dist < 5:\n",
    "                    frame[y, x, 0] = 1.0  # Red circle\n",
    "        \n",
    "        # Add a static square\n",
    "        frame[5:15, 5:15, 1] = 1.0  # Green square\n",
    "        \n",
    "        frames[i] = frame\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Generate synthetic video and process it\n",
    "synthetic_video = create_synthetic_video()\n",
    "# Uncomment to process the video\n",
    "# processed_results = process_existing_video(synthetic_video, noise_level=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49f699",
   "metadata": {},
   "source": [
    "## 8. Using Frame-Specific Conditioning\n",
    "\n",
    "The UNet3D model now supports both video-wide conditioning and optional frame-specific conditioning. Let's see how to use this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_frame_conditioning(num_frames=8, height=32, width=32, steps=20):\n",
    "    # Create batch\n",
    "    batch_size = 1\n",
    "    rng_gen = jax.random.PRNGKey(123)\n",
    "    rng_gen, key1, key2 = jax.random.split(rng_gen, 3)\n",
    "    \n",
    "    # Generate random global text embeddings\n",
    "    encoder_hidden_states = jax.random.normal(\n",
    "        key1, \n",
    "        shape=(batch_size, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate random frame-specific embeddings\n",
    "    frame_encoder_hidden_states = jax.random.normal(\n",
    "        key2, \n",
    "        shape=(batch_size, num_frames, 77, 64),\n",
    "        dtype=jnp.float32\n",
    "    )\n",
    "    \n",
    "    # Generate video frames - demonstrate with and without frame conditioning\n",
    "    print(f\"Generating {num_frames} frames with global conditioning only...\")\n",
    "    video_global = sampler.generate_images(\n",
    "        params=params,\n",
    "        num_images=batch_size,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        image_shape=(num_frames, height, width, 4),\n",
    "        model_conditioning_inputs=(encoder_hidden_states,),\n",
    "    )\n",
    "    \n",
    "    print(f\"Generating {num_frames} frames with global + frame-specific conditioning...\")\n",
    "    video_combined = sampler.generate_images(\n",
    "        params=params,\n",
    "        num_images=batch_size,\n",
    "        diffusion_steps=steps,\n",
    "        start_step=1000,\n",
    "        end_step=0,\n",
    "        priors=None,\n",
    "        image_shape=(num_frames, height, width, 4),\n",
    "        model_conditioning_inputs=(encoder_hidden_states, frame_encoder_hidden_states),\n",
    "    )\n",
    "    \n",
    "    return video_global, video_combined\n",
    "\n",
    "# Uncomment to run the experiment\n",
    "# video_global, video_combined = generate_with_frame_conditioning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6359d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_videos(video1, video2, title1=\"Global Conditioning\", title2=\"Global + Frame Conditioning\"):\n",
    "    # Normalize both videos\n",
    "    def normalize_video(video):\n",
    "        video_clip = np.array(video[0])\n",
    "        video_clip = (video_clip + 1.0) / 2.0\n",
    "        video_clip = np.clip(video_clip, 0.0, 1.0)\n",
    "        video_clip = video_clip[:, :, :, :3]  # RGB only\n",
    "        return video_clip\n",
    "    \n",
    "    video1_norm = normalize_video(video1)\n",
    "    video2_norm = normalize_video(video2)\n",
    "    \n",
    "    # Display side by side frames\n",
    "    num_frames = video1_norm.shape[0]\n",
    "    fig, axes = plt.subplots(2, num_frames, figsize=(num_frames*2, 4))\n",
    "    \n",
    "    # Display first video on top row\n",
    "    for i in range(num_frames):\n",
    "        axes[0, i].imshow(video1_norm[i])\n",
    "        axes[0, i].set_title(f\"Frame {i}\")\n",
    "        axes[0, i].axis('off')\n",
    "    axes[0, 0].set_ylabel(title1)\n",
    "    \n",
    "    # Display second video on bottom row\n",
    "    for i in range(num_frames):\n",
    "        axes[1, i].imshow(video2_norm[i])\n",
    "        axes[1, i].set_title(f\"Frame {i}\")\n",
    "        axes[1, i].axis('off')\n",
    "    axes[1, 0].set_ylabel(title2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to compare the videos\n",
    "# if 'video_global' in locals() and 'video_combined' in locals():\n",
    "#     compare_videos(video_global, video_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7057ecb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "1. How to initialize and use the FlaxUNet3DConditionModel\n",
    "2. How to generate new videos from random noise\n",
    "3. How to modify existing videos using the model\n",
    "4. How to use frame-specific conditioning for more detailed control\n",
    "\n",
    "The FlaxUNet3DConditionModel provides a powerful tool for video diffusion tasks, offering the performance benefits of JAX and Flax while maintaining compatibility with diffusers-style APIs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaxdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
