{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import augmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import grain.python as pygrain\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tqdm \n",
    "\n",
    "import fsspec\n",
    "import json\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer, FlaxCLIPTextModel, CLIPTextModel\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, load_from_disk\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import io\n",
    "import urllib\n",
    "\n",
    "import PIL.Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = get_datasets_user_agent()\n",
    "\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            request = urllib.request.Request(\n",
    "                image_url,\n",
    "                data=None,\n",
    "                headers={\"user-agent\": USER_AGENT},\n",
    "            )\n",
    "            with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "                image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "            break\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return image\n",
    "\n",
    "denormalizeImage = lambda x: (x + 1.0) * 127.5\n",
    "\n",
    "def plotImages(imgs, fig_size=(8, 8), dpi=100):\n",
    "    fig = plt.figure(figsize=fig_size, dpi=dpi)\n",
    "    imglen = imgs.shape[0]\n",
    "    for i in range(imglen):\n",
    "        plt.subplot(fig_size[0], fig_size[1], i + 1)\n",
    "        plt.imshow(jnp.astype(denormalizeImage(imgs[i, :, :, :]), jnp.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering pipeline for various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataMapper(map: Dict[str, Any]):\n",
    "    def _map(sample) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"url\": sample[map[\"url\"]],\n",
    "            \"caption\": sample[map[\"caption\"]],\n",
    "        }\n",
    "    return _map\n",
    "\n",
    "def imageFetcher():\n",
    "    def fetch_images(batch, num_threads, timeout=None, retries=0):\n",
    "        fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            batch[\"image\"] = list(executor.map(fetch_single_image_with_args, batch[\"url\"]))\n",
    "        return batch\n",
    "    return fetch_images\n",
    "\n",
    "def mapDataset(dataset, args, mapper=dataMapper, workers=16, batch_size=10000, should_remove_columns=True, fn_kwargs={}):\n",
    "    if should_remove_columns:\n",
    "        remove_columns = dataset.column_names\n",
    "    else:\n",
    "        remove_columns = None\n",
    "    return dataset.map(mapper(*args), batched=True, batch_size=batch_size, remove_columns=remove_columns, num_proc=workers, fn_kwargs=fn_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357e8fa8418439e8d2d0a8e23f3d1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/12096809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "laion12m6 = load_dataset(\"dclure/laion-aesthetics-12m-umap\")\n",
    "laion12m6_fused = laion12m6['train']\n",
    "laionMap = {\n",
    "    \"url\": \"URL\",\n",
    "    \"caption\": \"TEXT\",\n",
    "}\n",
    "laion12m6_fused = mapDataset(laion12m6_fused, (laionMap, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0571b694e010404390eee7a2ec5d2c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/18.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17680dbc7d224c59b499ace783120d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/591753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17c441f7d1b49b9ae7a7c35e5ad9645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/591753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mscoco = load_dataset(\"ChristophSchuhmann/MS_COCO_2017_URL_TEXT\", split=\"all\")\n",
    "mscoco_fused = mscoco\n",
    "mscocoMap = {\n",
    "    \"url\": \"URL\",\n",
    "    \"caption\": \"TEXT\",\n",
    "}\n",
    "mscoco_fused = mapDataset(mscoco_fused, (mscocoMap, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data = concatenate_datasets([mscoco_fused, mscoco_fused, mscoco_fused, laion12m6_fused, mscoco_fused, mscoco_fused])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15055574"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fused_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data = fused_data.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    sample = fused_data[i]\n",
    "    img = fetch_single_image(sample['url'])\n",
    "    if img is None:\n",
    "        print(\"Image is None\")\n",
    "        continue\n",
    "    text = sample['caption']\n",
    "    plt.imshow(img)\n",
    "    plt.title(text)\n",
    "    # print(f\"Aesthetic score: {sample['aesthetic_score_laion_v2']}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e9f3e6ae1743ca8fb8491dfab64ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/15055574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fused_data.save_to_disk(\"gs://flaxdiff-datasets-regional/datasets/laion-aesthetics-12m+mscoco-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_from_disk(\"gs://flaxdiff-datasets-regional/datasets/laion-aesthetics-12m+mscoco-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'caption'],\n",
       "    num_rows: 15055574\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaged_data = mapDataset(mscoco_fused, (), mapper=imageFetcher, batch_size=5000, workers=64, should_remove_columns=False, fn_kwargs={\"num_threads\": 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COYO-700M Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a01395a9cce4b2aa6b692d7299fa6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed77886a21de4cd79471379f4c2e2d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coyo700 = load_dataset(\"kakaobrain/coyo-700m\", num_proc=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseFilterMap = {\n",
    "    # \"word_count\": {\"min\": 0, \"max\": 100},\n",
    "    \"clip_similarity_vitl14\": {\"min\": 0.27, \"max\": 1000},\n",
    "    \"aesthetic_score_laion_v2\": {\"min\": 5.1, \"max\": 100},\n",
    "    \"watermark_score\": {\"min\": 0, \"max\": 0.4},\n",
    "}\n",
    "\n",
    "heavyFilterMap = {\n",
    "    # \"word_count\": {\"min\": 0, \"max\": 100},\n",
    "    \"clip_similarity_vitl14\": {\"min\": 0.26, \"max\": 100},\n",
    "    \"aesthetic_score_laion_v2\": {\"min\": 5.4, \"max\": 100},\n",
    "    \"watermark_score\": {\"min\": 0, \"max\": 0.8},\n",
    "    \"width\": {\"min\":256, \"max\":99999},\n",
    "    \"height\": {\"min\":256, \"max\":99999},\n",
    "}\n",
    "\n",
    "def coyoFilter(filterMap):\n",
    "    def _filter(sample):\n",
    "        for key, value in filterMap.items():\n",
    "            if sample[key] < value[\"min\"] or sample[key] > value[\"max\"]:\n",
    "                return False\n",
    "        return True\n",
    "    return _filter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d3eddced904acca1ddd5625e84d5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=64):   0%|          | 0/746972269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# goodCoyo700 = coyo700.filter(coyoFilter(baseFilterMap), num_proc=64)\n",
    "aestheticCoyo700 = coyo700.filter(coyoFilter(heavyFilterMap), num_proc=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24638115"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aestheticCoyo700['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    sample = aestheticCoyo700['train'][i]\n",
    "    img = fetch_single_image(sample['url'])\n",
    "    if img is None:\n",
    "        print(\"Image is None\")\n",
    "        continue\n",
    "    text = sample['text']\n",
    "    plt.imshow(img)\n",
    "    plt.title(text)\n",
    "    print(f\"Aesthetic score: {sample['aesthetic_score_laion_v2']}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0e12de501841ecb6f04de0f51383e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24638115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_data = mapDataset(aestheticCoyo700['train'], ({\n",
    "    \"url\":\"url\",\n",
    "    \"caption\":\"text\"\n",
    "    },),  batch_size=1000000, workers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09df57e712546dfb3de35bef97ca99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/24638115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_data.save_to_disk(\"gs://flaxdiff-datasets-regional/datasets/coyo700m-aesthetic-5.4_25M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laion12m6 = load_dataset(\"dclure/laion-aesthetics-12m-umap\", split=\"all\")\n",
    "# laion12m6_fused = laion12m6\n",
    "# laionMap = {\n",
    "#     \"url\": \"URL\",\n",
    "#     \"caption\": \"TEXT\",\n",
    "# }\n",
    "# laion12m6_fused = mapDataset(laion12m6_fused, (laionMap, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "from multiprocessing import Queue\n",
    "# from arrayqueues.shared_arrays import ArrayQueue\n",
    "# from faster_fifo import Queue\n",
    "import time\n",
    "import albumentations as A\n",
    "import queue\n",
    "\n",
    "data_queue = Queue(16*2000)\n",
    "\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    for _ in range(retries + 1):\n",
    "        try:\n",
    "            request = urllib.request.Request(\n",
    "                image_url,\n",
    "                data=None,\n",
    "                headers={\"user-agent\": USER_AGENT},\n",
    "            )\n",
    "            with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "                image = PIL.Image.open(io.BytesIO(req.read()))\n",
    "            break\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return image\n",
    "\n",
    "\n",
    "def default_image_processor(image, image_shape, interpolation=cv2.INTER_CUBIC):\n",
    "    image = A.longest_max_size(image, max(\n",
    "        image_shape), interpolation=interpolation)\n",
    "    image = A.pad(\n",
    "        image,\n",
    "        min_height=image_shape[0],\n",
    "        min_width=image_shape[1],\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        value=[255, 255, 255],\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def map_sample(\n",
    "    url, caption,\n",
    "    image_shape=(256, 256),\n",
    "    min_image_shape=(128, 128),\n",
    "    timeout=15,\n",
    "    retries=3,\n",
    "    upscale_interpolation=cv2.INTER_CUBIC,\n",
    "    downscale_interpolation=cv2.INTER_AREA,\n",
    "    image_processor=default_image_processor,\n",
    "):\n",
    "    try:\n",
    "        # Assuming fetch_single_image is defined elsewhere\n",
    "        image = fetch_single_image(url, timeout=timeout, retries=retries)\n",
    "        if image is None:\n",
    "            print(f\"Image is None {url}\")\n",
    "            return\n",
    "\n",
    "        image = np.array(image)\n",
    "        original_height, original_width = image.shape[:2]\n",
    "        # check if the image is too small\n",
    "        if min(original_height, original_width) < min(min_image_shape):\n",
    "            print(f\"Image too small {url}\")\n",
    "            return\n",
    "        # check if wrong aspect ratio\n",
    "        if max(original_height, original_width) / min(original_height, original_width) > 2.4:\n",
    "            print(f\"Wrong aspect ratio {url}\")\n",
    "            return\n",
    "        # check if the variance is too low\n",
    "        if np.std(image) < 1e-4:\n",
    "            print(f\"Low variance {url}\")\n",
    "            return\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        downscale = max(original_width, original_height) > max(image_shape)\n",
    "        interpolation = downscale_interpolation if downscale else upscale_interpolation\n",
    "\n",
    "        image = image_processor(\n",
    "            image, image_shape, interpolation=interpolation)\n",
    "        \n",
    "        print(f\"Processed {url}\")\n",
    "\n",
    "        data_queue.put({\n",
    "            \"url\": url,\n",
    "            \"caption\": caption,\n",
    "            \"image\": image,\n",
    "            \"original_height\": original_height,\n",
    "            \"original_width\": original_width,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}\", e)\n",
    "        # error_queue.put_nowait({\n",
    "        #     \"url\": url,\n",
    "        #     \"caption\": caption,\n",
    "        #     \"error\": str(e)\n",
    "        # })\n",
    "        pass\n",
    "\n",
    "\n",
    "def map_batch(\n",
    "    batch, num_threads=256, image_shape=(256, 256), \n",
    "    min_image_shape=(128, 128),\n",
    "    timeout=15, retries=3, image_processor=default_image_processor,\n",
    "    upscale_interpolation=cv2.INTER_CUBIC,\n",
    "    downscale_interpolation=cv2.INTER_AREA,\n",
    "):\n",
    "    try:\n",
    "        map_sample_fn = partial(map_sample, image_shape=image_shape, min_image_shape=min_image_shape,\n",
    "                                timeout=timeout, retries=retries, image_processor=image_processor,\n",
    "                                upscale_interpolation=upscale_interpolation,\n",
    "                                downscale_interpolation=downscale_interpolation)\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            executor.map(map_sample_fn, batch[\"url\"], batch['caption'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch\", e)\n",
    "        # error_queue.put_nowait({\n",
    "        #     \"batch\": batch,\n",
    "        #     \"error\": str(e)\n",
    "        # })\n",
    "        pass\n",
    "\n",
    "\n",
    "def parallel_image_loader(\n",
    "    dataset: Dataset, num_workers: int = 8, image_shape=(256, 256), \n",
    "    min_image_shape=(128, 128),\n",
    "    num_threads=256, timeout=15, retries=3, image_processor=default_image_processor,\n",
    "    upscale_interpolation=cv2.INTER_CUBIC,\n",
    "    downscale_interpolation=cv2.INTER_AREA,\n",
    "):\n",
    "    map_batch_fn = partial(map_batch, num_threads=num_threads, image_shape=image_shape, \n",
    "                           min_image_shape=min_image_shape,\n",
    "                           timeout=timeout, retries=retries, image_processor=image_processor,\n",
    "                           upscale_interpolation=upscale_interpolation,\n",
    "                           downscale_interpolation=downscale_interpolation)\n",
    "    shard_len = len(dataset) // num_workers\n",
    "    print(f\"Local Shard lengths: {shard_len}\")\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            # Repeat forever\n",
    "            shards = [dataset[i*shard_len:(i+1)*shard_len]\n",
    "                      for i in range(num_workers)]\n",
    "            print(f\"mapping {len(shards)} shards\")\n",
    "            pool.map(map_batch_fn, shards)\n",
    "            iteration += 1\n",
    "            print(f\"Shuffling dataset with seed {iteration}\")\n",
    "            dataset = dataset.shuffle(seed=iteration)\n",
    "            # Clear the error queue\n",
    "            # while not error_queue.empty():\n",
    "            #     error_queue.get_nowait()\n",
    "\n",
    "\n",
    "class ImageBatchIterator:\n",
    "    def __init__(\n",
    "        self, dataset: Dataset, batch_size: int = 64, image_shape=(256, 256), \n",
    "        min_image_shape=(128, 128),\n",
    "        num_workers: int = 8, num_threads=256, timeout=15, retries=3, \n",
    "        image_processor=default_image_processor,\n",
    "        upscale_interpolation=cv2.INTER_CUBIC,\n",
    "        downscale_interpolation=cv2.INTER_AREA,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        loader = partial(parallel_image_loader, num_threads=num_threads,\n",
    "                         image_shape=image_shape,\n",
    "                         min_image_shape=min_image_shape, \n",
    "                         num_workers=num_workers, \n",
    "                         timeout=timeout, retries=retries, image_processor=image_processor,\n",
    "                         upscale_interpolation=upscale_interpolation,\n",
    "                         downscale_interpolation=downscale_interpolation)\n",
    "        self.thread = threading.Thread(target=loader, args=(dataset,))\n",
    "        self.thread.start()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        def fetcher(_):\n",
    "            return data_queue.get()\n",
    "        with ThreadPoolExecutor(max_workers=self.batch_size) as executor:\n",
    "            batch = list(executor.map(fetcher, range(self.batch_size)))\n",
    "        return batch\n",
    "\n",
    "    def __del__(self):\n",
    "        self.thread.join()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "\n",
    "def default_collate(batch):\n",
    "    urls = [sample[\"url\"] for sample in batch]\n",
    "    captions = [sample[\"caption\"] for sample in batch]\n",
    "    images = np.stack([sample[\"image\"] for sample in batch], axis=0)\n",
    "    return {\n",
    "        \"url\": urls,\n",
    "        \"caption\": captions,\n",
    "        \"image\": images,\n",
    "    }\n",
    "\n",
    "\n",
    "def dataMapper(map: Dict[str, Any]):\n",
    "    def _map(sample) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"url\": sample[map[\"url\"]],\n",
    "            \"caption\": sample[map[\"caption\"]],\n",
    "        }\n",
    "    return _map\n",
    "\n",
    "\n",
    "class OnlineStreamingDataLoader():\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        image_shape=(256, 256),\n",
    "        min_image_shape=(128, 128),\n",
    "        num_workers=16,\n",
    "        num_threads=512,\n",
    "        default_split=\"all\",\n",
    "        pre_map_maker=dataMapper,\n",
    "        pre_map_def={\n",
    "            \"url\": \"URL\",\n",
    "            \"caption\": \"TEXT\",\n",
    "        },\n",
    "        global_process_count=1,\n",
    "        global_process_index=0,\n",
    "        prefetch=1000,\n",
    "        collate_fn=default_collate,\n",
    "        timeout=15,\n",
    "        retries=3,\n",
    "        image_processor=default_image_processor,\n",
    "        upscale_interpolation=cv2.INTER_CUBIC,\n",
    "        downscale_interpolation=cv2.INTER_AREA,\n",
    "    ):\n",
    "        if isinstance(dataset, str):\n",
    "            dataset_path = dataset\n",
    "            print(\"Loading dataset from path\")\n",
    "            if \"gs://\" in dataset:\n",
    "                dataset = load_from_disk(dataset_path)\n",
    "            else:\n",
    "                dataset = load_dataset(dataset_path, split=default_split)\n",
    "        elif isinstance(dataset, list):\n",
    "            if isinstance(dataset[0], str):\n",
    "                print(\"Loading multiple datasets from paths\")\n",
    "                dataset = [load_from_disk(dataset_path) if \"gs://\" in dataset_path else load_dataset(\n",
    "                    dataset_path, split=default_split) for dataset_path in dataset]\n",
    "            print(\"Concatenating multiple datasets\")\n",
    "            dataset = concatenate_datasets(dataset)\n",
    "            dataset = dataset.shuffle(seed=0)\n",
    "        dataset = dataset.map(pre_map_maker(pre_map_def), batched=True, batch_size=10000000)\n",
    "        self.dataset = dataset.shard(\n",
    "            num_shards=global_process_count, index=global_process_index)\n",
    "        print(f\"Dataset length: {len(dataset)}\")\n",
    "        self.iterator = ImageBatchIterator(self.dataset, image_shape=image_shape,\n",
    "                                           min_image_shape=min_image_shape,\n",
    "                                           num_workers=num_workers, batch_size=batch_size, num_threads=num_threads,\n",
    "                                            timeout=timeout, retries=retries, image_processor=image_processor,\n",
    "                                             upscale_interpolation=upscale_interpolation,\n",
    "                                             downscale_interpolation=downscale_interpolation)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Launch a thread to load batches in the background\n",
    "        self.batch_queue = queue.Queue(prefetch)\n",
    "\n",
    "        def batch_loader():\n",
    "            for batch in self.iterator:\n",
    "                try:\n",
    "                    self.batch_queue.put(collate_fn(batch))\n",
    "                except Exception as e:\n",
    "                    print(\"Error processing batch\", e)\n",
    "\n",
    "        self.loader_thread = threading.Thread(target=batch_loader)\n",
    "        self.loader_thread.start()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.batch_queue.get()\n",
    "        # return self.collate_fn(next(self.iterator))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = OnlineStreamingDataLoader(\"ChristophSchuhmann/MS_COCO_2017_URL_TEXT\", batch_size=16, num_workers=4, num_threads=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaxdiff.data.online_loader import OnlineStreamingDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multiple datasets from paths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating multiple datasets\n",
      "Dataset length: 15055574\n",
      "Local Shard lengths: 940973\n"
     ]
    }
   ],
   "source": [
    "dataloader = OnlineStreamingDataLoader([\n",
    "            \"gs://flaxdiff-datasets-regional/datasets/laion-aesthetics-12m+mscoco-2017\"\n",
    "        ], batch_size=16, num_workers=16, num_threads=512, default_split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.batch_queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(0, 100)):\n",
    "    batch = next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_loading(dataset):\n",
    "    dataset.map(map_batch_fn, num_proc=64, batched=True, batch_size=64, fn_kwargs={\"num_threads\": 64})\n",
    "    \n",
    "thread = threading.Thread(target=parallel_loading, args=(mscoco_fused,))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import aiohttp\n",
    "from io import BytesIO\n",
    "import asyncio\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    async def fetch_image(self, url):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                image_data = await response.read()\n",
    "                image = Image.open(BytesIO(image_data))\n",
    "                return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        url, caption = data['url'], data['caption']\n",
    "        loop = asyncio.get_event_loop()\n",
    "        image = loop.run_until_complete(self.fetch_image(url))\n",
    "        # Preprocess image and return along with the caption\n",
    "        image = image.resize((256, 256))  # Example resize\n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Example usage\n",
    "dataset = URLDataset(mscoco_fused)\n",
    "data_loader = DataLoader(dataset, batch_size=256, num_workers=8, prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(data_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        url = self.dataset[idx]['url']\n",
    "        caption = self.dataset[idx]['caption']\n",
    "        image = fetch_single_image(url)  # Assuming fetch_single_image is defined elsewhere\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"caption\": caption,\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Custom collation logic if needed\n",
    "    print(batch)\n",
    "    # urls = [item[\"url\"] for item in batch]\n",
    "    # fetch_single_image_with_args = partial(fetch_single_image, timeout=10, retries=3)\n",
    "    # with ThreadPoolExecutor(max_workers=len(batch)) as executor:\n",
    "    #     images = list(executor.map(fetch_single_image_with_args, urls))\n",
    "    \n",
    "    # return {\n",
    "    #     \"url\": urls,\n",
    "    #     \"caption\": [item[\"caption\"] for item in batch],\n",
    "    #     \"image\": images\n",
    "    # }\n",
    "    \n",
    "# Assuming mscoco_fused is your dataset\n",
    "dataset = CustomDataset(mscoco_fused)\n",
    "data_loader = DataLoader(dataset, batch_size=512, num_workers=8, collate_fn=collate_fn, prefetch_factor=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(data_loader):\n",
    "    # print(i)\n",
    "    # break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arrayqueues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Manager() as manager:\n",
    "img_queue = manager.Queue()\n",
    "process = multiprocessing.Process(target=parallel_image_loader, args=(mscoco_fused, img_queue, 8))\n",
    "process.start()\n",
    "process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import shared_memory\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datasets import Dataset\n",
    "import threading\n",
    "\n",
    "def create_shared_array(shape, dtype):\n",
    "    \"\"\"Create a shared numpy array.\"\"\"\n",
    "    nbytes = np.prod(shape) * np.dtype(dtype).itemsize\n",
    "    shm = shared_memory.SharedMemory(create=True, size=nbytes)\n",
    "    array = np.ndarray(shape, dtype=dtype, buffer=shm.buf)\n",
    "    return shm, array\n",
    "\n",
    "def map_fn(url, caption, shared_array, shared_index, lock, shape, dtype):\n",
    "    image = fetch_single_image(url)  # Assuming fetch_single_image is defined elsewhere\n",
    "    with lock:\n",
    "        index = shared_index.value\n",
    "        shared_array[index] = np.frombuffer(image, dtype=dtype).reshape(shape)  # Store image in shared memory\n",
    "        shared_index.value += 1  # Move to the next index\n",
    "        # Save additional info (url, caption) if necessary\n",
    "\n",
    "def map_batch_fn(batch, shared_array, shared_index, lock, shape, dtype, num_threads=64):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        executor.map(\n",
    "            map_fn, \n",
    "            batch[\"url\"], \n",
    "            batch['caption'], \n",
    "            [shared_array] * len(batch[\"url\"]), \n",
    "            [shared_index] * len(batch[\"url\"]), \n",
    "            [lock] * len(batch[\"url\"]), \n",
    "            [shape] * len(batch[\"url\"]), \n",
    "            [dtype] * len(batch[\"url\"])\n",
    "        )\n",
    "\n",
    "def parallel_image_loader(dataset: Dataset, shared_array, shared_index, lock, shape, dtype, num_workers: int = 8):\n",
    "    batch_len = len(dataset) // num_workers\n",
    "    batches = [dataset[i * batch_len:(i + 1) * batch_len] for i in range(num_workers)]\n",
    "    with multiprocessing.Pool(num_workers) as pool:\n",
    "        pool.starmap(\n",
    "            map_batch_fn, \n",
    "            [(batch, shared_array, shared_index, lock, shape, dtype) for batch in batches]\n",
    "        )\n",
    "\n",
    "class ImageBatchIterator:\n",
    "    def __init__(self, dataset: Dataset, num_workers: int = 8, batch_size: int = 64, image_shape=(224, 224, 3), dtype=np.uint8):\n",
    "        self.dataset = dataset\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.image_shape = image_shape\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Create shared memory array\n",
    "        self.shm, self.shared_array = create_shared_array((len(dataset),) + image_shape, dtype)\n",
    "        self.shared_index = multiprocessing.Value('i', 0)  # Shared index counter\n",
    "        self.lock = multiprocessing.Lock()  # Lock for safe indexing\n",
    "        \n",
    "        self.thread = threading.Thread(target=parallel_image_loader, args=(\n",
    "            dataset, self.shared_array, self.shared_index, self.lock, image_shape, dtype, num_workers))\n",
    "        self.thread.start()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.shared_index.value < self.batch_size:\n",
    "            raise StopIteration\n",
    "        \n",
    "        batch_start = max(0, self.shared_index.value - self.batch_size)\n",
    "        batch_end = self.shared_index.value\n",
    "        batch = self.shared_array[batch_start:batch_end]\n",
    "        return batch\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.thread.join()\n",
    "        self.shm.close()\n",
    "        self.shm.unlink()  # Free shared memory when done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "# Example usage:\n",
    "dataset = ImageBatchIterator(mscoco_fused, num_workers=16, batch_size=64, image_shape=(224, 224, 3))\n",
    "for i in tqdm.tqdm(range(0, 100)):\n",
    "    batch = next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(0, 100)):\n",
    "    batch = next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
